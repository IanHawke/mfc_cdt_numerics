[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Methods for MFC CDT",
    "section": "",
    "text": "Introduction\nThis book introduces the concepts needed for numerical modelling with climate applications in mind. These are the notes for the Numerical Methods course for the Mathematics for our Future Climate CDT.\nFor more detail, the most precise comparator would be (Durran 2010) which covers numerical methods for Geophysics. For high accuracy methods including spectral and finite element approaches the books of (Hesthaven 2017), (Hughes 2012), (Trefethen 1996) and (Boyd 2001) are all important and useful. For high speed flows where discontinuities matter the books of (LeVeque 2002), (LeVeque 1992) and (Toro 2013) are key texts.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#an-aside-on-computers",
    "href": "index.html#an-aside-on-computers",
    "title": "Numerical Methods for MFC CDT",
    "section": "An aside on computers",
    "text": "An aside on computers\nThe numerical methods we use and the design of the computers that we run the simulations on are closely linked. Changes in computer design (increases in memory, the move to parallel computing) can change which algorithms are most efficient. Sometimes computer architecture is specifically designed around particular algorithmic problems (such as graphical processing units - GPUs). Therefore it is important to have a loose understanding of how different parts of the algorithm interact with the computer architecture.\n\nKey steps\nUltimately we want the computer to do an arithmetic calculation, which is then repeated many (many!) times. A typical calculation would update a single value on a single point of a grid, where there may be multiple values on each of the many (many!) grid points. A computer will do a single calculation incredibly quickly, in one cycle, typically less than a nanosecond. However, in order to do the calculation the computer needs to know the values of the terms going in to the calculation. These will be stored in computer memory. If they are in cache memory then the values are “close” to the core doing the calculation and will be retrieved quickly (maybe 5-50 cycles). If they are in main memory then they will be retrieved more slowly (maybe 1,000 cycles). Main memory is much larger than cache memory, so only small calculations can use just the cache.\nIf we want to do extremely large calculations the the data will not fit in main memory of a single core or node. In this case we use parallel computing where the calculation is spread between different parts of a larger computer. In splitting the data this way we introduce artificial boundaries into the calculation, whose values need providing from somewhere. Communicating this data over the network is even slower again, taking maybe 100,000 cycles. The more parallel computing is needed, or the more values used in updating a single grid point, the more the communication cost increases.\nFinally, we may want to store data permanently on disk, or read complex data from a large table on disk. This is slower again, taking at least 1,000,000 cycles or more for large amounts of data. Careful choices of which data is stored and how can significantly improve the efficiency of a code.\nMore precise numbers on the efficiency and latency of computational operations can be found online. However, for numerical methods it is one of many concerns that need balancing. This course covers a range of numerical methods, from simple finite difference methods, more complex finite element methods designed to work on complex domains, and the high accuracy spectral methods. Whilst the low accuracy of finite differences compared to spectral methods may make them seem like a poor choice in theory, their simplicity can make it easier to use them efficiently, as the limited amount of data needed improves cache locality and reduces communication. Most production codes are the result of careful consideration of the trade-offs, and practical measurement of code efficiency.\n\n\n\n\nBoyd, John P. 2001. Chebyshev and Fourier Spectral Methods. Dover Books in Mathematics.\n\n\nDurran, Dale R. 2010. Numerical Methods for Fluid Dynamics: With Applications to Geophysics. Vol. 32. Springer Science & Business Media.\n\n\nHesthaven, Jan S. 2017. Numerical Methods for Conservation Laws: From Analysis to Algorithms. SIAM.\n\n\nHughes, T. J. R. 2012. The Finite Element Method: Linear Static and Dynamic Finite Element Analysis. Dover Civil and Mechanical Engineering. Dover Publications.\n\n\nLeVeque, Randall J. 1992. Numerical Methods for Conservation Laws. Vol. 214. Springer.\n\n\n———. 2002. Finite Volume Methods for Hyperbolic Problems. Vol. 31. Cambridge University Press.\n\n\nToro, Eleuterio F. 2013. Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction. Springer Science & Business Media.\n\n\nTrefethen, Lloyd Nicholas. 1996. “Finite Difference and Spectral Methods for Ordinary and Partial Differential Equations.” https://people.maths.ox.ac.uk/trefethen/pdetext.html.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "fd_1.html",
    "href": "fd_1.html",
    "title": "1  Climate models",
    "section": "",
    "text": "1.1 Navier-Stokes Equations\nThe standard models of gases and fluids rely on the Navier-Stokes equations. These express the conservation of energy, momentum, and particle number of the material coupled to gravity and allowing for sources of energy and diffusion. The form we will use can be written\n\\[\n\\begin{aligned}\n\\text{The Lagrangian derivative} && \\frac{D \\Psi}{D t} &= \\frac{\\partial \\Psi}{\\partial t} + \\mathbf{u} \\cdot \\nabla \\Psi \\\\\n\\text{Momentum} && \\frac{D \\mathbf{u}}{D t} &= -2 \\symbf{\\Omega} \\times \\mathbf{u} - \\frac{\\nabla p}{\\rho} + \\mathbf{g} + \\\\ &&& \\quad \\mu_u \\left( \\nabla^2 \\mathbf{u} + \\frac{1}{3} \\nabla (\\nabla \\cdot \\mathbf{u}) \\right) \\\\\n\\text{Continuity} && \\frac{D \\rho}{D t} + \\rho \\nabla \\cdot \\mathbf{u} &= 0 \\\\\n\\text{Energy} && \\frac{D \\theta}{D t} &= Q + \\mu_{\\theta} \\nabla^2 \\theta\n\\end{aligned}\n\\tag{1.1}\\]\nTo close the system we need to link the microphysical behaviour of the particles making up the material to the forces they impose, through an equation of state. An example would be the perfect gas law \\(p = \\rho R T\\).\nThe various symbols defined above are outlined in the following table.",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate models</span>"
    ]
  },
  {
    "objectID": "fd_1.html#sec-fd1-nse",
    "href": "fd_1.html#sec-fd1-nse",
    "title": "1  Climate models",
    "section": "",
    "text": "Symbol\nMeaning\n\n\n\n\n\\(\\mathbf{u}\\)\nWind vector\n\n\n\\(t\\)\nTime\n\n\n\\(\\symbf{\\Omega}\\)\nRotation rate of planet\n\n\n\\(\\rho\\)\nDensity of air\n\n\n\\(p\\)\nAtmospheric pressure\n\n\n\\(\\mathbf{g}\\)\nGravity vector (downwards)\n\n\n\\(\\theta\\)\nPotential temperature, \\(T (p_0/p)^{\\kappa}\\)\n\n\n\\(\\kappa\\)\nheat capacity ratio \\(\\simeq 1.4\\)\n\n\n\\(Q\\)\nSource of heat\n\n\n\\(\\mu_u, \\mu_{\\theta}\\)\nDiffusion coefficients",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate models</span>"
    ]
  },
  {
    "objectID": "fd_1.html#sec-fd1-swe",
    "href": "fd_1.html#sec-fd1-swe",
    "title": "1  Climate models",
    "section": "1.2 Shallow Water Equations",
    "text": "1.2 Shallow Water Equations\nThe Navier-Stokes equations are complex and accurately captures physics (such as acoustic waves) that has minimal impact on climate models, and whose numerical solution would impose problematic constraints on the cost and accuracy of the approximation. One simpler model is the shallow water equations (SWE). The assumptions needed to derive the SWE are\n\nHorizontal length scale \\(\\gg\\) vertical length scale;\nVery small vertical velocities.\n\nTo get the SWE, take the Navier-Stokes equations over orography and depth integrate. This gives the system\n\\[\n\\begin{aligned}\n    \\frac{D \\mathbf{u}}{D t} &= - \\symbf{\\Omega} \\times \\mathbf{u} - g \\nabla (h + h_0) + \\mu_u \\nabla^2 \\mathbf{u}, \\\\\n    \\frac{D h}{D t} + h \\nabla \\cdot \\mathbf{u} &= 0.\n\\end{aligned}\n\\tag{1.2}\\]\n\n\n\n\n\n\n\n\nFigure 1.1: A sketch of the vertical fluid direction which is integrated out in the derivation of the shallow water equations.\n\n\n\n\n\nIn the SWE the terms are\n\n\n\n\n\n\n\n\\(\\mathbf{u}\\)\nDepth integrated wind vector\n\n\n\\(t\\)\nTime\n\n\n\\(\\symbf{\\Omega}\\)\nRotation rate of planet\n\n\n\\(h\\)\nFluid depth\n\n\n\\(g\\)\nAcceleration due to gravity (in the direction of the depth integration)\n\n\n\\(\\nabla\\)\nGradients in the horizontal directions\n\n\n\\(h_0\\)\nheight of the bottom topography with respect to a reference point\n\n\n\\(\\mu_u\\)\nDiffusion of momentum\n\n\n\n\nExercise 1.1 Considering the meaning of the terms in the momentum equation in (Equation 1.1), what are the meanings of the terms of the momentum equation of the SWE?",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate models</span>"
    ]
  },
  {
    "objectID": "fd_1.html#sec-fd1-lin_swe",
    "href": "fd_1.html#sec-fd1-lin_swe",
    "title": "1  Climate models",
    "section": "1.3 Linearised Shallow Water Equations",
    "text": "1.3 Linearised Shallow Water Equations\nThe Shallow Water Equations are simpler than the Navier-Stokes equations but are still nonlinear. Dropping the nonlinear terms is an over-simplification for real models, but can be useful when developing methods. To linearise the SWE we assume that\n\n\\(\\mathbf{u} = (u, v, 0)^T\\) is small;\n\\(2 \\symbf{\\Omega} = (0, 0, f)^T\\);\n\\(h = H + h'\\), where \\(H\\) is uniform in space and time and \\(h'\\) is small;\nthe product of two small variables is ignored (even if one or both are inside a differential);\n\\(h_0\\) and \\(\\mu_u\\) are ignored.\n\nThis gives the linearised equations for \\(u, v\\), and \\(h'\\), expressed in terms of \\(f\\) (rather than \\(\\symbf{\\Omega}\\)), as\n\\[\n\\begin{aligned}\n    \\frac{\\partial u}{\\partial t} &= f v - g \\frac{\\partial h'}{\\partial x}, \\\\\n    \\frac{\\partial v}{\\partial t} &= -f u - g \\frac{\\partial h'}{\\partial y}, \\\\\n    \\frac{\\partial h'}{\\partial t} &= -H \\left( \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} \\right).\n\\end{aligned}\n\\tag{1.3}\\]",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate models</span>"
    ]
  },
  {
    "objectID": "fd_1.html#sec-fd1-advection",
    "href": "fd_1.html#sec-fd1-advection",
    "title": "1  Climate models",
    "section": "1.4 Advection",
    "text": "1.4 Advection\nIf the wind vector \\(\\mathbf{u}\\) is known (through, for example, the solution of the Navier-Stokes or Shallow Water equations) then we can solve for the transport of a property or particle that is moved by the fluid material. A standard example in climate modelling is pollution, where the concentration of the pollutant is represented by the scalar field \\(\\Psi\\). The pollutant then obeys the advection equation\n\\[\n\\underbrace{\\frac{D \\Psi}{D t}}_{(1)} = \\underbrace{\\frac{\\partial \\Psi}{\\partial t}}_{(2)} + \\underbrace{\\mathbf{u} \\cdot \\nabla \\Psi}_{(3)} = \\underbrace{S}_{(4)} + \\underbrace{\\mu_{\\Psi}\\nabla^2 \\Psi}_{(5)}.\n\\tag{1.4}\\]\nThe terms here are\n\nthe Lagrangian derivative of the pollution concentration,\nthe rate of change of the pollution concentration at a fixed point in space,\nthe advection of the pollution concentration by the wind velocity,\nthe source or sink of pollution concentration,\nthe diffusion of the pollution concentration.\n\nFrequently when we refer to the linear advection equation we mean the case with no source or sink of pollution, nor any diffusion. In this case we have\n\\[\n\\frac{D \\Psi}{D t} = \\frac{\\partial \\Psi}{\\partial t} + \\mathbf{u} \\cdot \\nabla \\Psi = 0.\n\\tag{1.5}\\]\nWe typically consider the wind velocity \\(\\mathbf{u}\\) to be a fixed function of time, and sometimes simplify further to make it spatially constant. Note now that if \\(\\nabla \\cdot \\mathbf{u} = 0\\) then the linear advection equation can be written in flux form\n\\[\n\\frac{\\partial \\Psi}{\\partial t} + \\nabla \\cdot \\left( \\Psi \\mathbf{u} \\right) = 0.\n\\tag{1.6}\\]\nThis is a special case of an equation in conservation law form,\n\\[\n\\frac{\\partial \\mathbf{q}}{\\partial t} + \\nabla \\cdot \\mathbf{f} \\left( \\mathbf{q} \\right) = 0,\n\\]\nfor which special numerical methods can be constructed.",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate models</span>"
    ]
  },
  {
    "objectID": "spectral.html",
    "href": "spectral.html",
    "title": "2  Spectral methods",
    "section": "",
    "text": "2.1 Advection expansions\nAs usual we will start with the one dimensional linear advection equation (Equation 1.5) in the form\n\\[\n\\frac{\\partial \\phi}{\\partial t} + u \\frac{\\partial \\phi}{\\partial x} = 0.\n\\tag{2.1}\\]\nWe assume the boundaries are periodic. We then approximate the solution as a truncated complex Fourier series\n\\[\n\\phi(x, t) = \\sum_{k = -N}^{N} a_k(t) \\exp(i k x) \\, .\n\\tag{2.2}\\]\nThe time dependence is completely contains within the coefficients \\(a_n(t)\\), which determine the solution. The number of modes used in the approximation, \\(2 N + 1\\), controls the accuracy of the method, and can be thought to be like the number of grid points in a finite difference method.\nThe series expansion substituted into the advection equation (Equation 2.1) gives\n\\[\n\\sum_{k = -N}^{N} \\left\\{ \\frac{\\text{d} a_k}{\\text{d} t} + i k u a_k \\right\\} \\exp(i k x) = 0.\n\\tag{2.3}\\]\nIn the simplest case where \\(u\\) is a constant, the orthogonality of the complex exponentials decouples all the modes to leave the ordinary differential equations\n\\[\n\\frac{\\text{d} a_k}{\\text{d} t} + i k u a_k = 0.\n\\tag{2.4}\\]\nThis can be solved explicitly and exactly giving \\(a_k = a_k(0) \\exp(-i k u t)\\). We therefore have the approximate solution\n\\[\n\\phi(x, t) = \\sum_{k = -N}^{N} a_k(0) \\exp[i k (x - u t)] \\, .\n\\tag{2.5}\\]\nThis solution has no dispersion error (all information propagates exactly at speed \\(u\\)) and its amplitude is constant. The error comes from projecting the exact initial data onto the basis functions, using a truncated series. This can have odd effects when \\(N\\) is small - strictly positive functions can appear to go negative - but for smooth data these problems converge very rapidly (as Fourier series approximations converge rapidly).",
    "crumbs": [
      "Spectral",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral methods</span>"
    ]
  },
  {
    "objectID": "spectral.html#sec-spectral-advection-nonuniform",
    "href": "spectral.html#sec-spectral-advection-nonuniform",
    "title": "2  Spectral methods",
    "section": "2.2 Non-uniform advection",
    "text": "2.2 Non-uniform advection\nWith finite difference and finite volume methods moving to non-uniform and nonlinear problems is “straightforward”, although their numerical analysis can be difficult. For example, if the advection velocity \\(u\\) is not constant so that \\(u = u(x)\\), the standard FTBS method (as seen earlier) is directly written as\n\\[\n\\phi^{n+1}_j = \\phi^n_j - \\frac{u_j \\, \\Delta t}{\\Delta x} \\left( \\phi^n_j - \\phi^n_{j-1} \\right) \\, .\n\\tag{2.6}\\]\nThe only change to the standard FTBS scheme is that the advection velocity is now evaluated at the update point, \\(u_j = u(x_j)\\). The CFL limit constraining the timestep now needs to maximize over all points \\(x_j\\), or equivalently over all advection velocities \\(u_j\\).\nThe spectral method, however, now becomes markedly more complex. As \\(u\\) varies in space we need to represent it by Fourier series expansion as well, as (for example)\n\\[\nu(x) = \\sum_{l = -N}^{N} u_l \\exp(i l x) \\, .\n\\tag{2.7}\\]\nWhen we substitute the series expansions for both the solution and the advection velocity into the advection equation we get\n\\[\n\\sum_{k = -N}^{N} \\left\\{ \\frac{\\text{d} a_k}{\\text{d} t} + i k a_k \\sum_{l = -N}^{N} u_l \\exp(i l x) \\right\\} \\exp(i k x) = 0.\n\\tag{2.8}\\]\nWhen we use orthogonality we find\n\\[\n\\frac{\\text{d} a_k}{\\text{d} t} + i \\sum_{\\substack{k+l = N \\\\ |k|, |l| \\le N}} k a_k u_l = 0 \\, .\n\\tag{2.9}\\]\nThis couples different modes (different values of \\(k\\)). We can no longer solve this ordinary differential equation exactly; instead we have to use some time differencing method to compute the result.\nThe results of applying a spectral method to non-constant advection can be seen in Figure 2.1. The left panel shows the solution after “half a period”, which is extremely well captured even with very few modes used (look at the solution near the right edge for the largest discrepancies). The convergence plot in the right panel is the key result, however. It shows how the error converges exponentially, which is far faster than any finite differencing scheme (which converges polynomially) can manage.\n\n\n\n\n\n\n\n\nFigure 2.1: A spectral method advects the initial data \\(\\sin^4(2 \\pi x)\\) around the domain \\(x \\in [0, 1]\\) up to \\(t = 0.5\\). Fourier modes \\(c_n = -N, \\dots, N\\) are used. Spectral (exponential) convergence with the number of modes is seen. It is also clear that this is much faster than first or second order convergence as indicated on the right panel.",
    "crumbs": [
      "Spectral",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral methods</span>"
    ]
  },
  {
    "objectID": "spectral.html#problems-and-solutions",
    "href": "spectral.html#problems-and-solutions",
    "title": "2  Spectral methods",
    "section": "2.3 Problems and solutions",
    "text": "2.3 Problems and solutions\nThe fundamental lesson of spectral methods for complex, nonlinear systems is given by Equation 2.9. That is, the approximate solution is updated by coupling every mode of the solution at the current time. This leads to extremely high accuracy but is associated with high costs. In particular, there are \\(\\mathcal{O}(N)\\) ordinary differential equations of the form (Equation 2.9) that need to be solved, and each requires \\(\\mathcal{O}(N)\\) terms to be evaluated within the sum, so each timestep costs \\(\\mathcal{O}(N^2)\\). For large numbers of coefficients this is much more expensive that a finite difference scheme.\nSome of these costs can be ameliorated by performing the nonlinear (or non-uniform) calculations in position space rather than frequency space. That is, the terms in the product \\(u \\partial_x \\phi\\) can be Fourier transformed back to position space, multiplied, and then transformed back. This is cheaper, reducing the cost to \\(\\mathcal{O}(N \\log N)\\). This idea is linked to spectral collocation methods.\nA related problem caused by all modes coupling is that of accuracy. When a mode with wavenumber \\(N\\) couples to another mode with wavenumber \\(N\\) then the true result should be a non-trivial evolution of the mode with wavenumber \\(2N\\). However, our truncated series approximation does not allow for this information to be captured. The information lost due to the nonlinear interactions at high wavenumber can have significant impacts, and particular techniques are needed to adjust for it. One standard method is to only use \\(2/3\\) of the coefficients when calculating the update terms. This (additionally) truncated approximation avoids aliasing problems due to the interactions.\nAnother, sometimes critical, problem, is related to the use of a single function basis expansion over the entire domain. In the example above we used a Fourier series on a periodic domain to represent the function \\(\\phi\\). However, when the function \\(\\phi\\) is discontinuous, it is well known that the Fourier series suffers from Gibbs oscillations. That is, the truncated Fourier series representation will oscillate (over- and under-shoot) around the discontinuity, and that the maximum error will not converge as the number of terms in the series increases. There is a lot of literature trying to make spectral methods work with discontinuous solutions, but in general it is not worth the effort: finite volume methods are more efficient for these problems.\nWhilst Durran (2010) gives an excellent introduction to spectral-type methods specifically for climate modelling, Boyd (2001) is the indespensable reference for spectral methods in general.\n\n\n\n\nBoyd, John P. 2001. Chebyshev and Fourier Spectral Methods. Dover Books in Mathematics.\n\n\nDurran, Dale R. 2010. Numerical Methods for Fluid Dynamics: With Applications to Geophysics. Vol. 32. Springer Science & Business Media.",
    "crumbs": [
      "Spectral",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral methods</span>"
    ]
  },
  {
    "objectID": "fe_1.html",
    "href": "fe_1.html",
    "title": "3  Finite Elements",
    "section": "",
    "text": "3.1 One dimension, time independent\nTake the advection-diffusion equation describing the motion of pollution concentration with a source of pollution, (Equation 1.4),\n\\[\n\\frac{D \\Psi}{D t} = \\frac{\\partial \\Psi}{\\partial t} + \\mathbf{u} \\cdot \\nabla \\Psi = S + \\mu_{\\Psi}\\nabla^2 \\Psi.\n\\tag{3.1}\\]\nFor now we will restrict to one spatial dimension and look for the steady state solution where the pollution generated by the source \\(S\\) is balanced by the diffusion term with coefficient \\(\\mu_{\\Psi} = \\mu\\), assuming that the wind velocity vanishes. We will also absorb the value of the diffusion coefficient \\(\\mu\\) into the source \\(S\\). Therefore the equation to solve is\n\\[\n0 = S + \\partial_{xx} \\Psi .\n\\tag{3.2}\\]\nTo be concrete we will assume that the domain on which we are solving is \\(x \\in [0, 1]\\), that the amount of pollution at the left boundary is fixed, and that the flux of pollution at the right boundary is also fixed,\n\\[\n\\Psi(0) = \\alpha, \\quad \\partial_x \\Psi |_{x=1} = \\beta.\n\\tag{3.3}\\]",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Elements</span>"
    ]
  },
  {
    "objectID": "fe_1.html#boundary-conditions",
    "href": "fe_1.html#boundary-conditions",
    "title": "3  Finite Elements",
    "section": "3.2 Boundary conditions",
    "text": "3.2 Boundary conditions\nAs finite element methods are designed to work on complex domains with complex boundaries, the boundary conditions are built in at the mathematical level. We need to consider the different types separately.\nWe remove the Dirichlet boundary condition (here at \\(x=0\\)) by writing\n\\[\n\\Psi(x) = \\psi(x) + q(x),\n\\tag{3.4}\\]\nwhere \\(q(x)\\) is a known function chosen so that \\(q(0) = \\alpha\\). That means that \\(\\psi(0) = 0\\), and \\(\\psi\\) satisfies homoegeneous boundary conditions. We will solve for \\(\\psi\\) and then put the boundary condition back in later. This can either be done globally (by making \\(q\\) a constant, or non-zero eveywhere), or locally (by making \\(q\\) non-zero only in a small region). The local approach is standard.\nNeumann boundaries, however, are built into the way the method works.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Elements</span>"
    ]
  },
  {
    "objectID": "fe_1.html#weak-form",
    "href": "fe_1.html#weak-form",
    "title": "3  Finite Elements",
    "section": "3.3 Weak form",
    "text": "3.3 Weak form\nNow we want to remove the second derivatives from the problem, as it is easier to reason about first derivatives alone. When working with finite volumes we saw that we could remove derivatives by integrating over the domain. However, we then ended up working with volume averaged quantities. To keep our steps more general, we first multiply by an arbitrary, smooth function \\(w(x)\\), and then integrate over the domain. As we already know the value of the solution at the left boundary due to the Dirichlet boundary condition, we can weight its value there to zero by enforcing \\(w(0) = 0\\).\nThe function \\(w(x)\\) is referred to as the weighting function. Using integration by parts the steady state advection diffusion equation becomes\n\\[\n\\begin{split}\n0 = \\left[  w(x) \\partial_x \\Psi(x) \\right]_0^1 - \\int_0^1  \\partial_x \\Psi(x) \\partial_x w(x) \\, \\text{d}x \\\\ + \\int_0^1 w(x) S(x) \\, \\text{d}x.\n\\end{split}\n\\tag{3.5}\\]\nWe introduce the “inner product” notation\n\\[\n(f, g) = \\int_0^1 f(x) g(x) \\, \\text{d}x\n\\tag{3.6}\\]\nand use the boundary conditions to give\n\\[\n(\\partial_x \\psi,  \\partial_x w) = w(1) \\beta - (\\partial_x q,  \\partial_x w) + ( w, S ).\n\\tag{3.7}\\]\nThis is the weak form of the equations. It is written in this fashion as the unknown term (\\(\\psi(x)\\)) is on the left hand side, but all terms on the right are either known (\\(\\beta, q, S\\)) or arbitrary (\\(w\\)). It can be proved that solutions of the strong form in (Equation 3.2) are also solutions of (Equation 3.7).",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Elements</span>"
    ]
  },
  {
    "objectID": "fe_1.html#function-representation",
    "href": "fe_1.html#function-representation",
    "title": "3  Finite Elements",
    "section": "3.4 Function representation",
    "text": "3.4 Function representation\nIn finite volume methods the domain is split into cells, or volumes, within which \\(\\Psi\\) is presented by a single number (its volume average). In finite element methods the domain is split into elements, which in many ways are indistinguishable from volumes, within which \\(\\Psi\\) and any other function is represented in terms of a series expansion. For example, we could choose within each element to represent \\(\\Psi\\) as a (truncated) Fourier series, or Taylor series.\nHowever, for practical purposes, we want to link the representations in neighbouring elements, but decouple the representations in elements that are not neighbours. To do this, we introduce shape or basis functions which are associated with the nodes of the grid.\nTo make this concrete, take our domain \\(x \\in [0, 1]\\) and split it into two elements \\(I_0 = [0, \\tfrac{1}{2}]\\) and \\(I_1 = [\\tfrac{1}{2}, 1]\\). The boundaries of the elemnts give us the three nodes \\(\\{ x_A \\} = \\{0, \\tfrac{1}{2}, 1\\}\\). Here \\({}_A\\) is a label - an integer labelling the nodes - which we count from \\(0\\) (so \\(A \\in \\{0, 1, 2\\}\\)). We then write the function of interest, \\(\\psi\\), as\n\\[\n\\psi(x) = \\sum_A \\psi_A N_A(x),\n\\tag{3.8}\\]\nwhere \\(N_A(x)\\) are the shape functions.\nWe choose \\(N_A(x)\\) to take the value \\(1\\) at node \\(x_A\\) and take the value \\(0\\) at any other node. This immediately means that \\(\\psi_A = \\psi(x_A)\\). Therefore the nodal values behave much like a finite difference representation.\nWe immediately note that our (approximate) solution process must compute, somehow, the values of \\(\\psi_A\\). Some are already known: the boundary condition at \\(x=0\\) in our case immediately implies that \\(\\psi_0 = 0\\). The other values must be fixed by the solution of (Equation 3.7).\nThere are now many choices we can make to fix the shape functions. The simplest is to choose the shape functions to be piecewise linear. This gives\n\\[\n\\begin{aligned}\nN_0(x) &= \\begin{cases} 1 - 2 x & 0 \\le x \\le 1/2 \\\\ 0 & 1/2 \\le x \\le 1 \\end{cases} \\\\\nN_1(x) &= \\begin{cases}  2 x & 0 \\le x \\le 1/2 \\\\ 2 - 2 x & 1/2 \\le x \\le 1 \\end{cases} \\\\\nN_2(x) &= \\begin{cases} 0 & 0 \\le x \\le 1/2 \\\\ 2 x - 1 & 1/2 \\le x \\le 1 \\end{cases}\n\\end{aligned}\n\\tag{3.9}\\]\n\n\n\n\n\n\n\n\nFigure 3.1: Shape functions for a domain with two elements (hence three nodes).\n\n\n\n\n\nWe now make the Galerkin assumption that the same function basis expansion is used for the unknown function \\(\\psi\\) and also for the test function \\(w\\). We write out \\(w\\) and \\(q\\) using the same shape functions. As noted above we enforce that \\(q\\) drops immediately to zero away from the boundary, which means we write\n\\[\nq(x) = \\alpha N_0(x).\n\\tag{3.10}\\]\nThis means the weak form (Equation 3.7) becomes\n\\[\n\\begin{split}\n\\sum_B w_B \\sum_A \\psi_A ( \\partial_x N_A,  \\partial_x N_B ) = w_{N_\\text{elements}} \\beta - \\\\\n\\alpha \\sum_B w_B ( \\partial_x N_0,  \\partial_x N_B ) +\n\\sum_B w_B ( N_B, S ) .\n\\end{split}\n\\tag{3.11}\\]\nThis has to be true for any choice of weight function \\(w\\), so for any choice of the coefficients \\(w_B\\). We gather terms as\n\\[\n\\sum_B w_B \\left\\{ \\sum_A K_{AB} \\psi_A - F_B \\right\\} = 0.\n\\tag{3.12}\\]\nTo hold for any choice of weight function the term in curly brackets must vanish. Here the stiffness matrix \\(K\\) and force vector \\(\\mathbf{F}\\) are independent of \\(\\psi\\). The steps here are very similar to those in the implicit finite difference methods such as BTCS. This gives\n\\[\nK \\symbf{\\psi} = \\mathbf{F},\n\\tag{3.13}\\]\nwhere the coefficients of the stiffness matrix \\(K\\) are given by\n\\[\nK_{AB} = \\int_0^1 \\partial_x N_A(x) \\partial_x N_B(x) \\, \\text{d}x\n\\tag{3.14}\\]\nand the coefficients of the force vector \\(\\mathbf{F}\\) are given by\n\\[\n\\begin{split}\nF_B = \\beta \\delta^{N_\\text{elements}}_B + \\int_0^1 N_B(x) S(x) \\, \\text{d}x \\, - \\alpha \\int_0^1 \\partial_x N_A(x) \\partial_x N_B(x) \\, \\text{d}x .\n\\end{split}\n\\tag{3.15}\\]\nHere \\(\\delta^C_B\\) is the Kronecker delta (zero except when \\(B \\equiv C\\), where it is one) and encodes the Neumann boundary value.\nNote that the final term in the force vector (which results from the Dirichlet boundary condition) has a very similar form to the entries of the stiffness matrix. However, the shape function \\(N_0\\) is non-zero at the boundary of the domain, which can cause issues with directly using the stiffness matrix here. For that, and other reasons, it is best to use the element viewpoint below.\n\nExercise 3.1  \n\nCompute (analytically) the coefficients of the stiffness matrix given the shape functions above in (Equation 3.9).\nCompute (analytically) the coefficients of the force vector when \\(S(x) = 1 - x\\).\nSolve (numerically) for \\(\\psi_A\\) and plot the resulting solution for \\(\\psi(x)\\). In the simplified case \\(\\alpha = 0 = \\beta\\) we can choose \\(q(x) \\equiv 0\\), so that \\(\\Psi = \\psi\\). Compare against the exact solution \\(\\Psi(x) = x (x^2 - 3 x + 3) / 6\\).",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Elements</span>"
    ]
  },
  {
    "objectID": "fe_1.html#the-element-viewpoint",
    "href": "fe_1.html#the-element-viewpoint",
    "title": "3  Finite Elements",
    "section": "3.5 The element viewpoint",
    "text": "3.5 The element viewpoint\nThis works surprisingly well given the small number of elements and associated nodes. However, as the sources get more complex we will need to work with more elements to improve accuracy. We need to go about this more systematically.\nFirst we note that in any element there are only two shape functions that are not zero. In the notation we have used so far element \\(I_A = [x_{A}, x_{A+1}]\\) and the two non-zero shape functions have been \\(N_{A}\\) and \\(N_{A+1}\\).\nSecond, we note that in general the elements will have different sizes. In higher dimensions this gets ever more complicated. However, by using a coordinate transformation, we can take any element from the interval \\([x_{A}, x_{A+1}]\\) to the interval \\(\\xi \\in [-1, 1]\\). We have\n\\[\n\\begin{aligned}\n\\xi(x) &= \\frac{2 x - x_{A} - x_{A+1}}{x_{A+1} - x_{A}}, \\\\\nx(\\xi) &= \\frac{(x_{A+1} - x_{A}) \\xi + x_{A} + x_{A+1}}{2} .\n\\end{aligned}\n\\tag{3.16}\\]\nWe can now write the two non-zero shape functions in terms of the reference coordinates \\(\\xi\\) as\n\\[\nN_a(\\xi) = \\tfrac{1}{2} (1 + \\xi_a \\xi), \\quad a = 1, 2.\n\\tag{3.17}\\]\nThe label \\(a\\) is labelling the shape functions within the reference element, written in terms of the reference coordinates. These labels can be linked back to the original shape functions in the end.\nNow, we remember that the weak form is written in terms of the stiffness matrix and force vector, and these depend on integrals of the shape functions and their derivatives. Computing the derivatives in the reference coordinates is straightforward,\n\\[\n\\partial_\\xi N_a = \\frac{(-1)^a}{2}.\n\\tag{3.18}\\]\nTo map this back to derivatives in the original coordinates we require a Jacobian, which needs the derivatives of the coordinate transformation. This needs\n\\[\n\\begin{aligned}\n\\partial_x \\xi &= \\frac{2}{x_{A+1} - x_{A}}, \\\\\n\\partial_\\xi x &= \\frac{x_{A+1} - x_{A}}{2}.\n\\end{aligned}\n\\tag{3.19}\\]\nWe can now compute the contribution that one single element \\(e = I_A\\) makes. This is\n\\[\n\\begin{aligned}\nk_{ab}^e & = \\int_{x_{A}}^{x_{A+1}} \\partial_x N_a \\partial_x N_b \\, \\text{d}x \\\\\n&= \\int_{-1}^1 \\partial_{\\xi} x  \\partial_x N_a \\partial_x N_b \\, \\text{d}\\xi \\\\\n&= \\int_{-1}^1 \\left(\\partial_{\\xi} x\\right)^{-1}  \\partial_\\xi N_a \\partial_\\xi N_b \\, \\text{d}\\xi \\\\\n&= \\frac{(-1)^{(a+b)}}{x_{A+1} - x_{A}}.\n\\end{aligned}\n\\tag{3.20}\\]\nThis is incredibly useful: there’s no need to do any integrals at all. Note that this gives a \\(2 \\times 2\\) matrix corresponding to a single element: to get the complete stiffness matrix we need to “add all these up”.\nThe element force vector is, in general, more complex, as it involves an integral over the complex source \\(S\\). However, we can approximate this by writing the source in terms of its values at the nodes as well, so\n\\[\nS(x) = \\sum_a S_a N_a(x),\n\\tag{3.21}\\]\ngiving \\(S_a = S(x(\\xi_a))\\). We can then compute, for the simple shape functions we use here,\n\\[\nf^e_a = \\frac{x_{A+1}-x_{A}}{6} \\begin{pmatrix} 2 f_1 + f_2 \\\\ f_1 + 2 f_2 \\end{pmatrix} \\, .\n\\tag{3.22}\\]\nFinally, we need to include the boundary condition terms in the force vector. To include the Neumann boundary condition at the right boundary we adjust the final entry,\n\\[\nF_{N_\\text{elements}} \\to F_{N_\\text{elements}} + \\beta.\n\\tag{3.23}\\]\nTo include the Dirichlet boundary condition at the left boundary we adjust the first entry, which needs adjusting using \\(\\int \\partial_x N_0 \\partial_x N_B\\). For the linear shape functions chosen here this is only non-zero within the first element, so we can use the local stiffness matrix to adjust the first entry,\n\\[\nF_0 \\to F_0 - \\alpha k^0_{12}.\n\\tag{3.24}\\]\n(Note that the numbering here has the first element number \\(e=0\\) and the local labels \\(a \\in \\{1, 2\\}\\))",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Elements</span>"
    ]
  },
  {
    "objectID": "fe_1.html#linking-elements-to-equations",
    "href": "fe_1.html#linking-elements-to-equations",
    "title": "3  Finite Elements",
    "section": "3.6 Linking elements to equations",
    "text": "3.6 Linking elements to equations\nOur goal is to construct a linear (matrix) equation to give us the solution \\(\\psi_A\\) at all nodes \\(A\\) where it isn’t enforced by the boundary conditions (which, in the example so far, is all nodes except the left-hand boundary). We note that each interior node is linked to two elements, so contributions from the element matrix will affect more than one equation.\nTo keep track of this, we construct the location matrix or location array \\(LM\\) which, given the node number \\(a \\in \\{1, 2\\}\\) and the element number \\(e\\) returns the associated equation number.\nAny node that is not to be included (as its value is given by a Dirichlet boundary condition) has its associated equation number \\(A\\) set to \\(-1\\). The first node that must be included is given value \\(0\\). We then go element-by-element: the left-hand node of element \\(e\\) is the same as the right-hand node of element \\(e-1\\), so picks up the same equation number. The right-hand node of element \\(e\\), if considered, then has equation number one higher than the left-hand node of that element. This translates directly into Python code:\n\nN_elements = 4 # for example\n\nLM = np.zeros((2, N_elements), dtype=np.int64)\nfor e in range(N_elements):\n    if e==0:\n        # Treat first element differently due to BC\n        LM[0, e] = -1 # Left hand node of first element\n                      # is not considered thanks to BC.\n        LM[1, e] = 0 # the first equation\n    else:\n        # Left node of this element is \n        # right node of previous element\n        LM[0, e] = LM[1, e-1]\n        LM[1, e] = LM[0, e] + 1\n\nNow the global stiffness matrix and force vector can be assembled: for each element \\(e\\) we construct the element \\(k^e_{ab}\\) and \\(f^e_b\\) and add the appropriate components, as\n\\[\n\\begin{aligned}\nK_{LM(a, e)\\,LM(b,e)} &= K_{LM(a, e)\\,LM(b,e)} + k^e_{ab} & a, b &\\in \\{1, 2\\}, \\\\\nf_{LM(b, e)} &= f_{LM(b, e)} + f^e_b & b &\\in \\{1, 2\\}.\n\\end{aligned}\n\\]\nNote that we need one more structure to keep track of the boundary conditions. As noted above, if a node is on a boundary then the value of the force vector needs modifying, either by including its value directly (in the case of a Neumann boundary) or by using some appropriate multiple of the local stiffness matrix (in the case of a Dirichlet boundary). This structure must map the node number to the value in the boundary condition; the location matrix can be used to check the boundary condition type.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Elements</span>"
    ]
  },
  {
    "objectID": "fe_1.html#algorithm",
    "href": "fe_1.html#algorithm",
    "title": "3  Finite Elements",
    "section": "3.7 Algorithm",
    "text": "3.7 Algorithm\nThis gives our full algorithm:\n\nSet the number of elements \\(N_{\\text{elements}}\\).\nSet node locations \\(x_A\\), where \\(A = 0, \\dots, N_{\\text{elements}}\\).\nSet up the location matrix \\(LM\\).\nSet up a boundary value structure (in Python a dictionary would work).\nSet up arrays, initially all zero, for the global stiffness matrix (size \\(N_{\\text{elements}} \\times N_{\\text{elements}}\\)) and for vector (size \\(N_{\\text{elements}}\\)).\nFor each element:\n\nForm the element stiffness matrix \\(k^e_{ab}\\).\nForm the element force vector \\(f^e_b\\).\nAdd the contributions to the global stiffness matrix and force vector.\nModify using the boundary values if needed.\n\nSolve \\(K \\symbf{\\psi} = \\mathbf{F}\\).\n\n\nExercise 3.2 Write a finite element solver for the problem above, as a function that takes as input the number of elements and the source function \\(S\\), as well as the boundary conditions \\(\\alpha, \\beta\\). It should use a uniformly spaced grid, and return the nodes \\(x_A\\) and the solution at the nodes \\(\\Psi_A = \\psi_A + q_A\\).\nCheck that the function returns the same result as above when used with two elements and \\(S(x) = 1 - x\\).\nThe apply the solver to the case \\(S(x) = (1 - x)^2\\) with exact solution \\(\\Psi(x) = x (4 - 6 x + 4 x^2 - x^3) / 12\\). Compute the 2-norm of the error and check how it converges with resolution.\nFinally check that the solver works on the case\n\\[\nS(x) = \\begin{cases}\n1 & |x - \\tfrac{1}{2}| &lt; \\tfrac{1}{4}, \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\tag{3.25}\\]\nwith boundary conditions\n\\[\n\\alpha = \\Psi(0) = 0.1, \\quad \\beta = \\partial_x \\Psi(1) = -0.2.\n\\tag{3.26}\\]\nThe exact solution in this case is\n\\[\n\\Psi = \\begin{cases}\n0.3 x + 0.1 & x &lt; \\tfrac{1}{4} \\\\\n-\\tfrac{1}{2} x^2 + 0.55 x + \\tfrac{11}{160} & \\tfrac{1}{4} &lt; x &lt; \\tfrac{3}{4} \\\\\n-0.2 x + 0.35 & x &gt; \\tfrac{3}{4}\n\\end{cases}.\n\\tag{3.27}\\]\n\n\n\n\n\n\nTip\n\n\n\nIt will be useful for later purposes to write helper functions that compute the global coordinates from the reference coordinates, and compute the elements matrices and vectors from the nodes.\n\n\n\n\n\n\n\nHughes, T. J. R. 2012. The Finite Element Method: Linear Static and Dynamic Finite Element Analysis. Dover Civil and Mechanical Engineering. Dover Publications.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Elements</span>"
    ]
  },
  {
    "objectID": "fe_2.html",
    "href": "fe_2.html",
    "title": "4  Two dimensions",
    "section": "",
    "text": "4.1 Elements\nWe now need to split the domain into subdomains - elements. Constructing a good grid for a general case is a hard problem for which there are many complex solvers available. In our case we are going to use one simple approach: triangulate the domain by using equal sized triangles.\nFigure 4.1: The square domain split into triangular elements. Red dots are the nodes. Red squares contain the element numbers. Green squares contain the global node numbers. Blue squares contain the local node numbers, with the associated element number as subscript.\nWhat we’re doing here is\nWe have that for element \\(e\\) and local node number \\(a = 0, 1, 2\\) the global node number is \\(A = IEN(e, a)\\). This notation is sufficiently conventional that matplotlib recognizes it with its triplot/tripcolor/trisurf functions. In this case we have\nIEN = np.array([[0, 1, 2], \n                [1, 3, 2]])\nwhich says that the first element (element 0) is made of the (global) nodes numbered 0, 1, and 2, which the second element (element 1) is made of the (global) nodes numbered 1, 3, and 2. It is convention that the nodes are ordered in the anti-clockwise direction as the local number goes from 0 to 2.\nThe plot shows the\nWe will need one final array, which is the \\(ID\\) or (integer) destination array. This links the global node number to the global equation number in the final linear system. As the order of the equations in a linear system doesn’t matter, this essentially encodes whether a node should have any equation in the linear system. Any node on \\(\\Gamma_D\\), where the value of the temperature is given, should not have an equation. In the example above the left edge is fixed, so nodes 0 and 2 lie on \\(\\Gamma_D\\) and should not have an equation. Thus in our case we have\nID = np.array([-1, 0, -1, 1])\nIn the one dimensional case we used the location matrix or \\(LM\\) array to link local node numbers in elements to equations. With the \\(IED\\) and \\(ID\\) arrays the \\(LM\\) matrix is strictly redundant, as \\(LM(a, e) = ID(IEN(e, a))\\). However, it’s still standard to construct it:\nLM = np.zeros_like(IEN.T)\nfor e in range(IEN.shape[0]):\n    for a in range(IEN.shape[1]):\n        LM[a,e] = ID[IEN[e,a]]\nLM\n\narray([[-1,  0],\n       [ 0,  1],\n       [-1, -1]])",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two dimensions</span>"
    ]
  },
  {
    "objectID": "fe_2.html#elements",
    "href": "fe_2.html#elements",
    "title": "4  Two dimensions",
    "section": "",
    "text": "Providing a list of nodes by their global coordinates.\nProviding the (integer) element node array IEN which says how the elements are linked to the nodes.\n\n\n\n\n\n\nelement numbers in the red boxes\nthe global node numbers in the green boxes\nthe local element numbers in the blue boxes (the subscript shows the element number).",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two dimensions</span>"
    ]
  },
  {
    "objectID": "fe_2.html#function-representation-and-shape-functions",
    "href": "fe_2.html#function-representation-and-shape-functions",
    "title": "4  Two dimensions",
    "section": "4.2 Function representation and shape functions",
    "text": "4.2 Function representation and shape functions\nWe’re going to want to write our unknown functions \\(\\Psi, w\\) in terms of shape functions. These are easiest to write down for a single reference element, in the same way as we did for the one dimensional case where our reference element used the coordinates \\(\\xi\\). In two dimensions we’ll use the reference coordinates \\(\\xi_1, \\xi_2\\), and the standard “unit” triangle shown in (Figure 4.2).\n\n\n\n\n\n\n\n\nFigure 4.2: The standard reference triangle.\n\n\n\n\n\nThe shape functions on this triangle are\n\\[\n\\begin{aligned}\n  N_0(\\xi_1, \\xi_2) &= 1 - \\xi_1 - \\xi_2, \\\\\n  N_1(\\xi_1, \\xi_2) &= \\xi_1, \\\\\n  N_2(\\xi_1, \\xi_2) &= \\xi_2.\n\\end{aligned}\n\\tag{4.7}\\]\nThe derivatives are all either \\(0\\) or \\(\\pm 1\\).\nAs soon as we have the shape functions, our weak form becomes\n\\[\n\\begin{split}\n  \\sum_A T_A \\int_{\\Omega} \\text{d}\\Omega \\, \\left( \\partial_{x} N_A (x, y) \\partial_{x} N_B(x, y) + \\partial_{y} N_A(x, y) \\partial_{y} N_B(x, y) \\right) = \\\\ \\int_{\\Omega} \\text{d}\\Omega \\, N_B(x, y) f(x, y).\n\\end{split}\n\\tag{4.8}\\]\nIf we restrict to a single element the weak form becomes\n\\[\n\\begin{split}\n  \\sum_A T_A \\int_{\\triangle} \\text{d}\\triangle \\, \\left( \\partial_{x} N_A (x, y) \\partial_{x} N_B(x, y) + \\partial_{y} N_A(x, y) \\partial_{y} N_B(x, y) \\right) = \\\\ \\int_{\\triangle} \\text{d}\\triangle \\, N_B(x, y) f(x, y).\n\\end{split}\n\\tag{4.9}\\]\nWe need to map the triangle and its \\((x, y) = \\mathbf{x}\\) coordinates to the reference triangle and its \\((\\xi_1, \\xi_2) = \\symbf{\\xi}\\) coordinates. We also need to work out the integrals that appear in the weak form. We need the transformation formula\n\\[\n  \\int_{\\triangle} \\text{d}\\triangle \\, \\phi(x, y) = \\int_0^1 \\text{d}\\xi_2 \\, \\int_0^{1-\\xi_2} \\text{d}\\xi_1 \\, \\phi \\left( x(\\xi_1, \\xi_2), y(\\xi_1, \\xi_2) \\right) j(\\xi_1, \\xi_2),\n\\tag{4.10}\\]\nwhere the Jacobian matrix \\(J\\) is\n\\[\n  J = \\left[ \\frac{\\partial \\mathbf{x}}{\\partial \\symbf{\\xi}} \\right] = \\begin{pmatrix} \\partial_{\\xi_1} x & \\partial_{\\xi_2} x \\\\ \\partial_{\\xi_1} y & \\partial_{\\xi_2} y \\end{pmatrix}\n\\tag{4.11}\\]\nand hence the Jacobian determinant \\(j\\) is\n\\[\n  j = \\det{J} = \\det \\left[ \\frac{\\partial \\mathbf{x}}{\\partial \\symbf{\\xi}} \\right] = \\det \\begin{pmatrix} \\partial_{\\xi_1} x & \\partial_{\\xi_2} x \\\\ \\partial_{\\xi_1} y & \\partial_{\\xi_2} y \\end{pmatrix}.\n\\tag{4.12}\\]\nWe will also need the Jacobian matrix when writing the derivatives of the shape functions in terms of the coordinates on the reference triangle, i.e.\n\\[\n  \\begin{pmatrix} \\partial_x N_A & \\partial_y N_A \\end{pmatrix} = \\begin{pmatrix} \\partial_{\\xi_1} N_A & \\partial_{\\xi_2} N_A \\end{pmatrix} J^{-1} .\n\\tag{4.13}\\]\nThe integral over the reference triangle can be directly approximated using, for example, Gauss quadrature. To second order we have\n\\[\n\\begin{split}\n  \\int_0^1 \\text{d}\\xi_2 \\, \\int_0^{1-\\xi_2} \\text{d}\\xi_1 \\, \\psi \\left( x(\\xi_1, \\xi_2), y(\\xi_1, \\xi_2) \\right) \\simeq \\\\ \\frac{1}{6} \\sum_{j = 1}^{3} \\psi \\left( x((\\xi_1)_j, (\\xi_2)_j), y((\\xi_1)_j, (\\xi_2)_j) \\right)\n\\end{split}\n\\tag{4.14}\\]\nwhere\n\\[\n\\begin{aligned}\n  (\\xi_1)_1 &= \\frac{1}{6}, & (\\xi_2)_1 &= \\frac{1}{6}, \\\\\n  (\\xi_1)_2 &= \\frac{4}{6}, & (\\xi_2)_2 &= \\frac{1}{6}, \\\\\n  (\\xi_1)_3 &= \\frac{1}{6}, & (\\xi_2)_3 &= \\frac{4}{6}.\n\\end{aligned}\n\\tag{4.15}\\]\nFinally, we need to map from the coordinates \\(\\symbf{\\xi}\\) to the coordinates \\(\\mathbf{x}\\). This is straightforward if we think of writing each component \\((x, y)\\) in terms of the shape functions. So for element \\(e\\) with node locations \\((x^e_a, y^e_a)\\) for local node number \\(a = 0, 1, 2\\) we have\n\\[\n\\begin{aligned}\n  x &= x^e_0 N_0(\\xi_1, \\xi_2) + x^e_1 N_1(\\xi_1, \\xi_2) + x^e_2 N_2(\\xi_1, \\xi_2), \\\\\n  y &= y^e_0 N_0(\\xi_1, \\xi_2) + y^e_1 N_1(\\xi_1, \\xi_2) + y^e_2 N_2(\\xi_1, \\xi_2).\n\\end{aligned}\n\\tag{4.16}\\]",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two dimensions</span>"
    ]
  },
  {
    "objectID": "fe_2.html#algorithm",
    "href": "fe_2.html#algorithm",
    "title": "4  Two dimensions",
    "section": "4.3 Algorithm",
    "text": "4.3 Algorithm\nThe steps needed to solve this case closely follow the algorithm in one dimension. The outline algorithm becomes\n\nSet up the grid, including the mapping between elements and nodes (IEN) and between elements and equations (ID).\nSet up a boundary value structure (in Python a dictionary would work).\nSet up the location matrix \\(LM\\).\nSet up arrays, initially all zero, for the global stiffness matrix and for vector.\nFor each element:\n\nForm the element stiffness matrix \\(k^e_{ab}\\).\nForm the element force vector \\(f^e_b\\).\nAdd the contributions to the global stiffness matrix and force vector.\nModify using the boundary values if needed.\n\nSolve \\(K \\symbf{\\psi} = \\mathbf{F}\\).\n\nThe key difference is the complexity of mapping from a general element (triangle) to the reference element (triangle) on which all the coefficients are known. The key steps are:\n\nWrite a function that, given \\(\\symbf{\\xi}\\), returns that shape functions at that location.\nWrite a function that, given \\(\\symbf{\\xi}\\), returns the derivatives of the shape functions at that location.\nWrite a function that, given the (global) locations \\(\\mathbf{x}\\) of the nodes of a triangular element and the local coordinates \\(\\symbf{\\xi}\\) within the element returns the corresponding global coordinates.\nWrite a function that, given the (global) locations \\(\\mathbf{x}\\) of the nodes of a triangular element and the local coordinates \\(\\symbf{\\xi}\\), returns the Jacobian matrix at that location.\nWrite a function that, given the (global) locations \\(\\mathbf{x}\\) of the nodes of a triangular element and the local coordinates \\(\\symbf{\\xi}\\), returns the determinant of the Jacobian matrix at that location.\nWrite a function that, given the (global) locations \\(\\mathbf{x}\\) of the nodes of a triangular element and the local coordinates \\(\\symbf{\\xi}\\) within the element returns the derivatives \\(\\partial_\\mathbf{x} N_a = J^{-1} \\partial_{\\symbf{\\xi}} N_a\\).\nWrite a function that, given a function \\(\\psi({\\symbf{\\xi}})\\), returns the quadrature of \\(\\psi\\) over the reference triangle.\nWrite a function that, given the (global) locations of the nodes of a triangular element and a function \\(\\phi(x, y)\\), returns the quadrature of \\(\\phi\\) over the element.\nWrite a function to compute the coefficients of the stiffness matrix for a single element, \\[\n  k^e_{ab} = \\int_{\\triangle^e}  \\text{d}\\triangle^e \\, \\left( \\partial_{x} N_a (x, y) \\partial_{x} N_b(x, y) + \\partial_{y} N_a(x, y) \\partial_{y} N_b(x, y) \\right).\n\\]\nWrite a function to compute the coefficients of the force vector for a single element, \\[\n  f^e_b = \\int_{\\triangle^e} \\text{d}\\triangle^e \\, N_b(x, y) f(x, y).\n\\tag{4.17}\\]",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two dimensions</span>"
    ]
  },
  {
    "objectID": "fe_2.html#grid-generation",
    "href": "fe_2.html#grid-generation",
    "title": "4  Two dimensions",
    "section": "4.4 Grid generation",
    "text": "4.4 Grid generation\nThe final, essential, topic that has not been covered is how to generate a grid. Good grid generators or meshers are generally hard (look at, for example, gmesh or dmsh for examples): here is a very simple one for this specific problem.\n\ndef generate_2d_grid(Nx):\n    Nnodes = Nx+1\n    x = np.linspace(0, 1, Nnodes)\n    y = np.linspace(0, 1, Nnodes)\n    X, Y = np.meshgrid(x,y)\n    nodes = np.zeros((Nnodes**2,2))\n    nodes[:,0] = X.ravel()\n    nodes[:,1] = Y.ravel()\n    ID = np.zeros(len(nodes), dtype=np.int64)\n    boundaries = dict()  # Will hold the boundary values\n    n_eq = 0\n    for nID in range(len(nodes)):\n        if np.allclose(nodes[nID, 0], 0):\n            ID[nID] = -1\n            boundaries[nID] = 0  # Dirichlet BC\n        else:\n            ID[nID] = n_eq\n            n_eq += 1\n            if ( (np.allclose(nodes[nID, 1], 0)) or \n                 (np.allclose(nodes[nID, 0], 1)) or \n                 (np.allclose(nodes[nID, 1], 1)) ):\n                boundaries[nID] = 0 # Neumann BC\n    IEN = np.zeros((2*Nx**2, 3), dtype=np.int64)\n    for i in range(Nx):\n        for j in range(Nx):\n            IEN[2*i+2*j*Nx  , :] = (i+j*Nnodes, \n                                    i+1+j*Nnodes, \n                                    i+(j+1)*Nnodes)\n            IEN[2*i+1+2*j*Nx, :] = (i+1+j*Nnodes, \n                                    i+1+(j+1)*Nnodes, \n                                    i+(j+1)*Nnodes)\n    return nodes, IEN, ID, boundaries\n\nThe results of using a more complex mesh generator (in this case dmsh) on a more complex domain, but still solving the heat equation using exactly the functions outlined in the exercise below, is shown in (Figure 4.3).\n\n\n\n\n\n\n\n\nFigure 4.3: A more complex grid generated by dmsh for a more general polygonal domain. The heat equation is solved with a Gaussian source. The boundary at \\(x=0\\) is held fixed at \\(\\Psi=0\\). All other boundaries use Neumann boundary conditions where the flux vanishes. The function that solves the finite element method here is identical to that on the simpler grids.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two dimensions</span>"
    ]
  },
  {
    "objectID": "fe_2.html#exercise",
    "href": "fe_2.html#exercise",
    "title": "4  Two dimensions",
    "section": "4.5 Exercise",
    "text": "4.5 Exercise\n\nExercise 4.1  \n\nWrite a function that, given a list of nodes and the \\(IEN\\) and \\(ID\\) arrays, and also given the source function \\(S\\), uses the finite element method to return \\(\\symbf{\\Psi}\\).\nTest on the system \\(S(x, y) = 1\\) with exact solution \\(\\Psi = x(1-x/2)\\).\nFor a more complex case with the same boundary conditions try \\[\n  S(x, y) = 2 x (x - 2) (3 y^2 - 3 y + \\tfrac{1}{2}) + y^2 (y - 1)^2\n\\tag{4.18}\\]\n\nwith exact solution\n\\[\n  \\Psi(x, y) = x (1 - \\tfrac{x}{2}) y^2 (1 - y)^2.\n\\tag{4.19}\\]",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Two dimensions</span>"
    ]
  },
  {
    "objectID": "finite_elements_3.html",
    "href": "finite_elements_3.html",
    "title": "5  Time evolution",
    "section": "",
    "text": "5.1 Weak form\nWe will repeat the key steps from the static case: introducing the weak form by multiplying by a test function and integrating over the domain, and representing all functions in terms of shape, or basis, functions. The key difference now is that the functions depend on time.\nThe continuum weak form of (Equation 5.2) is\n\\[\n\\begin{split}\n\\underbrace{\\int w \\frac{\\partial \\Psi}{\\partial t}}_{\\text{(1)}} + \\underbrace{u w(1) \\Psi(1)}_{\\text{(2a)}} - \\underbrace{u \\int \\Psi \\frac{\\partial w}{\\partial x}}_{\\text{(2b)}} = \\underbrace{\\int w S}_{\\text{(3)}} + \\\\ \\underbrace{\\mu_{\\Psi} w(1) \\beta}_{\\text{(4a)}} - \\underbrace{\\mu_{\\Psi} \\int \\frac{\\partial w}{\\partial x} \\frac{\\partial \\Psi}{\\partial x}}_{\\text{(4b)}} .\n\\end{split}\n\\tag{5.4}\\]\nNote that we have used that the weighting function vanishes at the Dirichlet boundary, \\(w(0) = 0\\), and assumed that the test function \\(w\\) is time independent. We have integrated by parts again to move derivatives onto the test function, with the notation linking the boundary terms to their associated integrals.\nTo include time dependence we take the static case of (Equation 3.8), which was\n\\[\n\\Psi(x) = \\sum_A \\Psi_A N_A(x),\n\\tag{5.5}\\]\nand generalise it so that \\(\\Psi_A \\equiv \\Psi_A(t)\\). Now the nodal values \\(\\Psi_A\\) contain the time dependence. The shape functions \\(N_A\\) remain time independent.\nThe weak form will now give us\n\\[\n\\begin{aligned}\n\\text{(1)} & \\to \\sum_B w_B \\sum_A \\frac{\\partial \\Psi_A}{\\partial_t}  \\int N_A N_B, \\\\\n\\text{(2a)} & \\to u w(1) \\Psi(1) , \\\\\n\\text{(2b)} & \\to -u \\sum_B w_B \\sum_A \\Psi_A \\int N_A \\frac{\\partial N_B}{\\partial x}, \\\\\n\\text{(3)} & \\to \\sum_B w_B \\int N_B S, \\\\\n\\text{(4a)} & \\to \\mu_{\\Psi} w(1) \\beta, \\\\\n\\text{(4b)} & \\to  - \\mu_{\\Psi} \\sum_B w_B \\sum_A \\Psi_A \\int \\frac{\\partial N_A}{\\partial x} \\frac{\\partial N_B}{\\partial x}  .\n\\end{aligned}\n\\tag{5.6}\\]\nAs before, we gather together all of the terms with respect to the (arbitrary) coefficients of the test function \\(w_B\\). We also introduce the function \\(q(x) = \\alpha N_0(x)\\) and write \\(\\Psi(x, t) = \\psi(x, t) + q(x)\\), to balance the Dirichlet boundary condition. This gives the matrix equation\n\\[\nM_{AB} \\frac{\\partial \\psi_A}{\\partial t} + K_{AB} \\psi_A = F_B,\n\\tag{5.7}\\]\nwhere \\(M\\) is the mass or capacity matrix, and \\(K\\) is the stiffness matrix and \\(\\mathbf{F}\\) the force vector as before. We read off the coefficients of the various terms using (Equation 5.6) as\n\\[\n\\begin{aligned}\nM_{ab} &= \\int N_a N_b, \\\\\nK_{ab} &= \\mu_{\\Psi} \\int \\frac{\\partial N_a}{\\partial x} \\frac{\\partial N_b}{\\partial x} - u \\int N_a  \\frac{\\partial N_b}{\\partial x}, \\\\\nF_b &= \\int N_b S - u \\Psi_0 \\delta^0_{B(b)} + \\mu_{\\Psi} \\delta_{B(b)}^{N_\\text{elements}} - q \\delta^0_{B(b)} k^0_{12} .\n\\end{aligned}\n\\tag{5.8}\\]\nThe terms are given with respect to the local element number \\(\\{a, b\\}\\) and are assembled into the global matrix in exactly the same way as before. The notation \\(B(b)\\) indicates the global node number computed from the local number, and is needed to identify the boundary locations.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Time evolution</span>"
    ]
  },
  {
    "objectID": "finite_elements_3.html#time-stepping",
    "href": "finite_elements_3.html#time-stepping",
    "title": "5  Time evolution",
    "section": "5.2 Time stepping",
    "text": "5.2 Time stepping\nThe matrix equation (Equation 5.7) solves for \\(\\psi\\) via evolving its nodal values \\(\\psi_A\\) in time. This equation is semi-discrete: it is continuous in time, but discrete in space.\nSemi-discrete approaches to PDEs are more general than finite element methods. They have the huge advantage that we end up solving ODEs, for which there is a vast literature, considerable analysis, and many well implemented and tested codes. They have the disadvantage that they are typically less efficient than bespoke, fully discrete methods with the same order of accuracy.\nHere we give two simple methods that are often used in solving the time evolution of a semi-discrete problem. We write the system to be solved as\n\\[\n\\frac{\\partial \\mathbf{U}}{\\partial t} = \\symbf{\\mathcal{F}} (\\mathbf{U})\n\\tag{5.9}\\]\nwhere \\(\\mathbf{U}\\) is the state vector. In the case of (Equation 5.7) the state vector is the coefficients \\(\\psi_A\\) and the right-hand-side vector \\(\\symbf{\\mathcal{F}}\\) is\n\\[\n\\mathcal{F}_A = M^{-1}_{AB} \\left( F_B - K_{AB} \\psi_A \\right).\n\\tag{5.10}\\]\nNote that explicitly computing the matrix inverse is typically numerically inaccurate, and instead solving the linear system is preferred.\n\n5.2.1 Euler\nWe denote the (known) state vector at time \\(t^{(n)}\\) and \\(\\mathbf{U}^n\\). In Euler’s method we update to the unknown time \\(t^{(n+1)} = t^{(n)} + \\Delta t\\) by\n\\[\n\\mathbf{U}^{n+1} = \\mathbf{U}^n + \\Delta t \\, \\symbf{\\mathcal{F}} \\left( \\mathbf{U}^n \\right).\n\\tag{5.11}\\]\nThis is exactly the result given by forward differencing in time, as seen (for example) in the derivation of FTCS. It is explicit, and gives first order accuracy in time.\n\n\n5.2.2 RK2\nHigher order methods can be constructed by updating in multiple stages. At each stage we compute the right hand side vector, and from that can approximate a solution at a time typically in \\([t^{(n)}, t^{(n+1)}]\\). By combining these stages appropriately a better approximation to the solution at the next timestep can be found. The Runge-Kutta family of methods is typical for this approach.\nOne particular subset of Runge-Kutta methods can ensure a form of total variation boundedness, linked to the TVD methods seen earlier. These Strict Stability Preserving (SSP) methods are preferred when dealing with semi-discrete PDE evolutions. The standard second order, explicit, SSP Runge-Kutta method (ESSPRK2) can be written\n\\[\n\\begin{aligned}\n\\mathbf{U}^{[1]} &= \\mathbf{U}^{n} + \\Delta t \\,  \\symbf{\\mathcal{F}} \\left( \\mathbf{U}^n \\right), \\\\\n\\mathbf{U}^{n+1} &= \\tfrac{1}{2} \\left\\{ \\mathbf{U}^{n} +  \\mathbf{U}^{[1]} + \\Delta t \\, \\symbf{\\mathcal{F}} \\left( \\mathbf{U}^{[1]} \\right) \\right\\} .\n\\end{aligned}\n\\tag{5.12}\\]",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Time evolution</span>"
    ]
  },
  {
    "objectID": "finite_elements_3.html#evolving-to-steady-state",
    "href": "finite_elements_3.html#evolving-to-steady-state",
    "title": "5  Time evolution",
    "section": "5.3 Evolving to steady state",
    "text": "5.3 Evolving to steady state\nLet us take the simplified problem from (Chapter 3), where\n\\[\nS = 1 - x, \\quad \\mu = 1.\n\\tag{5.13}\\]\nWe know the steady state solution is \\(x (x^2 - 3 x + 3) / 6\\). However, we can start from the solution \\(x = 0\\) and see how it evolves towards steady state. For this we will use Euler timestepping.\n\n\n\n\n\n\n\n\nFigure 5.1: Evolving to the steady state solution to the advection diffusion equation with source \\(S = 1 - x\\).",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Time evolution</span>"
    ]
  },
  {
    "objectID": "finite_elements_3.html#advection-dominated-flow",
    "href": "finite_elements_3.html#advection-dominated-flow",
    "title": "5  Time evolution",
    "section": "5.4 Advection dominated flow",
    "text": "5.4 Advection dominated flow\nLet us now look at the advection problem from setting \\(S(x) = 0 = \\mu_\\Psi\\). The advection equation results and we have, from (Equation 5.8) that\n\\[\n\\begin{aligned}\nM_{ab} &= \\int N_a N_b, \\\\\nK_{ab} &= - u \\int N_a  \\frac{\\partial N_b}{\\partial x}, \\\\\nF_b &= - u \\Psi_0 \\delta^0_{B(b)} - q \\delta^0_{B(b)} k^0_{12} .\n\\end{aligned}\n\\tag{5.14}\\]\nThe results are seen in (Figure 5.2) where the expected advective behaviour is seen. Note that the boundary conditions are built into the scheme here, through their appearance in the force vector. Changing boundary conditions will require modifying the scheme.\n\n\n\n\n\n\n\n\nFigure 5.2: Advection of a sine wave. Note the standard boundary conditions (Dirichlet on the left, Neumann on the right) means the pulse leaves the domain.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Time evolution</span>"
    ]
  },
  {
    "objectID": "finite_elements_3.html#matrix-structure",
    "href": "finite_elements_3.html#matrix-structure",
    "title": "5  Time evolution",
    "section": "5.5 Matrix structure",
    "text": "5.5 Matrix structure\nThe properties of the finite element method is closely tied to the properties of the matrices involved.\n\n\n\n\n\n\n\n\nFigure 5.3: The structure of the mass (left) and stiffness (right) matrices for the advection diffusion problem with \\(u=1, \\mu=0.1\\). The symmetry and sparsity can both be used theoretically and practically.\n\n\n\n\n\nIn (Figure 5.3) we see the structure of the mass and stiffness matrices for the one dimensional advection-diffusion problem. The matrices are symmetric and sparse - in this case they are tridiagonal. In higher dimensions, or with more complex boundary conditions, or with more complex systems of equations, the matrices become messier. However, the key point of matrix sparsity will be retained.\nThe symmetry of the mass matrix is useful in proving invertibility (essential for the method to work). The symmetry of the stiffness matrix is useful for constructing the eigenvalues of the discrete system, which are linked to the amplification rates in a von Neumann stability analysis.\nThe sparsity of the matrices is crucial in implementing a method that scales to large numbers of elements. In the simplest one dimensional implementation the size of the matrices scales as \\(N_{\\text{elements}}^2\\). However, the tridiagonal nature means that the amount of useful (non-zero) information scales as \\(N_{\\text{elements}}\\). The computational memory and work saved is substantial even with only tens of elements. In higher dimensions with large domains (millions of elements) the construction of the full matrix is impractical.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Time evolution</span>"
    ]
  },
  {
    "objectID": "finite_elements_4.html",
    "href": "finite_elements_4.html",
    "title": "6  Flexibility with efficiency",
    "section": "",
    "text": "6.1 Function basis and weak form\nRecall that in this chapter we are looking at the advection equation\n\\[\n\\Psi_t + u \\Psi_x = 0.\n\\tag{6.1}\\]\nWe want to be able to compute the value of the function \\(\\Psi(t, x)\\) at any point. We can do this by writing \\(a\\) in terms of a function basis \\(\\phi_n(t, x)\\) as\n\\[\n\\Psi(t, x) = \\sum_n \\hat{\\Psi}_n \\phi_n(t, x).\n\\tag{6.2}\\]\nHere the modes or modal coefficients \\(\\hat{\\Psi}_n\\) are constants. An example of a function basis would be\n\\[\n  \\begin{aligned}\n    \\phi_{0}(t, x) &= 1, & \\phi_{1}(t, x) &= x, & \\phi_{2}(t, x) &= t, \\\\\n    \\phi_{3}(t, x) &= \\tfrac{1}{2} x^2, & \\phi_{4}(t, x) &= \\tfrac{1}{2} t^2, &\n    \\phi_{5}(t, x) &= x t.\n  \\end{aligned}\n\\tag{6.3}\\]\nThese six modes will perfectly describe any function that remains quadratic for all space and time.\nNote that the function basis plays a very similar role to the shape functions discussed earlier for other finite element methods. The crucial distinctions here are that (a) the function basis is confined to a single element whilst a shape function is linked to a node and can be non-zero in multiple elements, and (b) shape functions are chosen so that the coefficients are directly linked to the values of the function at nodes, whilst basis functions are typically not normalized in that way.\nIt is often more convenient the explicitly separate space and time, as we saw using the semi-discrete approach in (Chapter 5). In this case we can represent the solution using a purely spatial function basis, as\n\\[\n  \\Psi(t, x) = \\sum_n \\hat{\\Psi}_n(t) \\phi_n(x).\n\\tag{6.4}\\]\nNow the modes depend on time, and there will only be three basis functions needed to describe quadratic data.\nClearly we cannot store an infinite number of modes. By restricting our sum to the \\(m+1\\) modes by writing\n\\[\n  \\Psi(t, x) = \\sum_{n=0}^{m} \\hat{\\Psi}_n(t) \\phi_n(x)\n\\tag{6.5}\\]\nwe are restricting out solution to live in a finite dimensional function space (denoted \\(\\mathbb{V}\\)) with basis \\(\\{ \\phi_n \\}, \\ n = 0, \\dots, m\\). That means that, in general, any solution \\(\\Psi(t, x)\\) will have an error when plugged into the advection equation. We can pick out a solution by insisting that this error is orthogonal to \\(\\mathbb{V}\\).\nTo see how this works, write the error term as \\(\\epsilon(t, x)\\). As our (infinite dimensional) function basis can describe any function, we expand the error in terms of the \\(\\phi_n\\) as well, as\n\\[\n  \\epsilon(t, x) = \\sum_n \\hat{\\epsilon}_n(t) \\phi_n(x).\n\\tag{6.6}\\]\nTherefore our advection equation, including the error term, becomes\n\\[\n  \\sum_n \\left[ \\left( \\frac{\\partial \\hat{\\Psi}_n}{\\partial t} - \\hat{\\epsilon}_n \\right) \\phi_n(x) +\n  u \\hat{\\Psi}_n \\frac{\\partial \\phi_n}{\\partial x}(x) \\right] = 0.\n\\tag{6.7}\\]\nAs our solution is finite dimensional this can be written as\n\\[\n    \\sum_{n=0}^{m} \\left[ \\frac{\\partial \\hat{\\Psi}_n}{\\partial t} \\phi_n(x) +\n    u \\hat{\\Psi}_n \\frac{\\partial \\phi_n}{\\partial x}(x) \\right] =\n    \\sum_{n=m+1}^{\\infty} \\hat{\\epsilon}_n \\phi_n(x).\n\\tag{6.8}\\]\nWe have used here that the orthogonality of the error requires \\(\\hat{\\epsilon}_n\\) does not contribute for \\(n = 0, \\dots, m\\). Using standard linear algebra techniques (as \\(\\phi_n\\) is a basis), we can get individual equations by taking the inner product with another member of the basis. If we were dealing with vectors in \\(\\mathbb{R}^n\\) then the inner product would be a vector dot product. As we are dealing with functions the inner product requires multiplication and integration over the domain,\n\\[\n  \\langle f(x), \\phi_l(x) \\rangle = \\int_V \\text{d} x \\, f(x) \\phi_l(x).\n\\tag{6.9}\\]\nThis will also write the conservation law in the integral, weak, form. This leads to, after integrating by parts,\n\\[\n  \\begin{split}\n    \\sum_{n=0}^{m}  \\left[ \\frac{\\partial \\hat{\\Psi}_n}{\\partial t} \\left( \\int_V \\text{d} x \\ \\phi_n(x) \\phi_l(x) \\right) + \\int_{\\partial V} u \\hat{\\Psi}_n \\phi_n(x) \\phi_l(x) - \\right. \\\\\n    \\left.  u  \\hat{\\Psi}_n \\int_V \\text{d} x \\, \\phi_n \\frac{\\partial \\phi_l}{\\partial x}(x) \\right] = \\sum_{n=m+1}^{\\infty}  \\hat{\\epsilon}_n \\int_V \\text{d} x\\, \\phi_n(x) \\phi_l(x).\n  \\end{split}\n\\tag{6.10}\\]\nRestricting ourselves to the first \\(m+1\\) modes we see only the left hand side contributes.\nWe can write this result as a matrix equation. Define the state vector\n\\[\n  \\symbf{\\hat{\\Psi}} = (\\hat{\\Psi}_0, \\dots, \\hat{\\Psi}_N)^T.\n\\tag{6.11}\\]\nFor now, restrict to one dimension and set \\(V = [-1, 1]\\): we can use a coordinate transformation to convert to other domains. Then define the matrices\n\\[\n\\begin{aligned}\n  \\hat{M}_{ln} &= \\int_{-1}^1 \\phi_l(x) \\phi_n(x), \\\\\n  \\hat{S}_{ln} &= \\int_{-1}^1 \\phi_l(x) \\frac{\\partial \\phi_n}{\\partial x}(x),\n\\end{aligned}\n\\tag{6.12}\\]\nwhich can be pre-calculated and stored for repeated use. These are typically referred to (building on finite element work) as the mass matrix (\\(\\hat{M}\\)) and the stiffness matrix (\\(\\hat{S}\\)). We therefore finally have\n\\[\n  \\hat{M} \\frac{\\partial \\symbf{\\hat{\\Psi}}}{\\partial t} + \\hat{S}^T \\left( u \\symbf{\\hat{\\Psi}} \\right) =\n  -\\left[ \\symbf{\\phi} F \\right]_{-1}^1.\n\\tag{6.13}\\]\nThe right hand side term is the boundary flux and requires coupling to neighbouring cells, or boundary conditions. It requires evaluating a product of basis functions \\(\\phi_l(x) \\phi_n(x)\\) at the boundary of the domain.\nWe see that, once we have evaluated the mass and stiffness matrices, we can then update all modes \\(\\symbf{\\hat{\\Psi}}\\) by evaluating the boundary flux term on the right hand side and solving a linear system. This illustrates the small stencil of discontinuous Galerkin schemes: the only coupling to the other cells is through that boundary integral, which only couples to direct neighbours. However, if the flux terms couple different modes (as evaluating them requires evaluating a product of basis functions \\(\\phi_l(x) \\phi_n(x)\\)), then the amount of information communicated may still be large. Therefore the communication cost of the scheme is linked to the properties of the basis functions at the domain boundary.\nWe also see that the behaviour of the scheme will crucially depend on the mass matrix \\(\\hat{M}\\). If it is singular the scheme cannot work. If it is poorly conditioned then the scheme will rapidly lose accuracy. Crucially, with the monomial basis of (Equation 6.3), the condition number of the mass matrix grows very rapidly, and the scheme loses accuracy for moderate \\(m\\).\nThe choice of whether to prioritize the behaviour of the mass matrix or the flux terms leads to two different schemes.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Flexibility with efficiency</span>"
    ]
  },
  {
    "objectID": "finite_elements_4.html#sec-dg_modal",
    "href": "finite_elements_4.html#sec-dg_modal",
    "title": "6  Flexibility with efficiency",
    "section": "6.2 Modal Discontinuous Galerkin",
    "text": "6.2 Modal Discontinuous Galerkin\nIf we prioritize the behaviour of the mass matrix as the most important starting point for our scheme we are led to the modal Discontinuous Galerkin approach. We noted above that the choice of a monomial basis led to a poorly conditioned mass matrix. Instead, it is sensible to pick as a function basis something from the class of orthogonal polynomials, where\n\\[\n  \\int_V w(x) \\phi_l(x) \\phi_n(x) \\propto \\delta_{ln}.\n\\tag{6.14}\\]\nThe Kronecker delta \\(\\delta_{ln}\\) ensures that the mass matrix is diagonal, and hence always easy to invert. When the weight function \\(w(x)\\) is identically \\(1\\), as needed for the mass matrix in (Equation 6.12), this suggests we should use the Legendre polynomials \\(\\phi_n(x) = P_n(x)\\), which obey\n\\[\n  \\int_{-1}^1 P_l(x) P_n(x) = \\frac{2}{2 n + 1} \\delta_{ln}.\n\\tag{6.15}\\]\nA further simplification comes from choosing the normalized Legendre polynomials\n\\[\n  \\tilde{P}_n(x) = \\sqrt{\\frac{2 n + 1}{2}} P_n(x)\n\\tag{6.16}\\]\nwhich ensures that the mass matrix \\(\\hat{M}\\) is the identity matrix.\nNow that we have fixed a choice of basis functions we can evaluate the mass matrix (which will be the identity here) and the stiffness matrix \\(\\hat{S}\\). We still need to evaluate the boundary flux. If we explicitly write out equation (Equation 6.13) in index form (using Einstein summation convention over \\(n\\)) we have\n\\[\n  \\hat{M}_{ln} \\frac{\\partial \\hat{\\Psi}_n}{\\partial t} + \\hat{S}^T_{ln} u \\hat{\\Psi}_n = -\\left[ u P_l(x) P_n(x) \\hat{\\Psi}_n \\right]_{-1}^1.\n\\tag{6.17}\\]\nWe can now directly use that \\(P_n(1) = 1\\) and \\(P_n(-1) = (-1)^n\\) to get the boundary flux term as\n\\[\n  -\\left[ u P_l(x) P_n(x) \\hat{\\Psi}_n \\right]_{-1}^1 = u \\left\\{ (-1)^{l+n} \\Psi_n(-1) - \\Psi_n(1) \\right\\}.\n\\tag{6.18}\\]\nHere \\(\\Psi_n(1)\\), for example, is the \\(n^{\\text{th}}\\) mode of the solution at the boundary. As there are two solutions at the boundary of the element - the solution at \\(x = 1_{-}\\) from the interior of the element, and the solution at \\(x = 1_{+}\\) from the exterior (either another element, or from the boundary conditions), we need a Riemann solver to give us a single solution at \\(x=1\\). In the case of linear advection, as here, we can use the upwind solver for the modes as well as for the solution, so\n\\[\n  \\Psi_n(x; \\Psi^{-}_n, \\Psi^{+}_n) = \\begin{cases}\n    \\Psi^{-}_n & \\text{if } u \\ge 0 \\\\ \\Psi^{+}_n & \\text{otherwise.}\n\\end{cases}\n\\tag{6.19}\\]\nTwo points should be immediately noted about this discontinuous Galerkin method. First, if we restrict to only one mode (\\(N=0\\)), then the only basis function we have is \\(\\tilde{P}_0(x) = 1 / \\sqrt{2}\\), the mass matrix \\(\\hat{M} = 1\\), the stiffness matrix vanishes, and the boundary flux term reduces to the standard finite volume update. In general, the zero mode corresponds to the integral average over the cell or element.\nSecond, we note that the boundary flux term always couples different modes (when including more than just one), and only in the linear case will it be simple to give a flux formula that works for all modes. As the boundary flux term is crucial in many cases, we need to change approach to simplify the calculation of this term using (possibly approximate) solutions to the Riemann problem.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Flexibility with efficiency</span>"
    ]
  },
  {
    "objectID": "finite_elements_4.html#sec-dg_nodal",
    "href": "finite_elements_4.html#sec-dg_nodal",
    "title": "6  Flexibility with efficiency",
    "section": "6.3 Nodal Discontinuous Galerkin",
    "text": "6.3 Nodal Discontinuous Galerkin\nA problem with the modal form used above is with the boundary flux term. The solution (for nonlinear equations) of the flux for higher order modes is complex. The mode coupling at the boundary also means the amount of information communicated could be large, meaning the scheme is not as efficient as it could be. Instead we note that in standard finite volume schemes we need the value of the function either side of the interface. This suggests that, rather than using a modal expansion as above, we should use a nodal expansion where the values of the functions are known at particular points. If two of those points are at the boundaries of the cell then those values can be used to compute the flux.\nLet us denote these nodal locations by \\(\\xi_i\\), and the values of the solution at these locations by \\(\\Psi_i\\). We therefore have our solution in the form\n\\[\n  \\Psi(t, x) = \\sum_{i=0}^m \\Psi_i(t) \\ell_i(x)\n\\tag{6.20}\\]\nwhere the \\(\\ell_i(x)\\) are the standard indicator interpolating polynomials that obey\n\\[\n  \\ell_i(\\xi_j) = \\delta_{ij}.\n\\tag{6.21}\\]\nThis directly matches the modal form of the solution from (Equation 6.5),\n\\[\n  \\Psi(t, x) = \\sum_n \\hat{\\Psi}_n(t) \\phi_n(x),\n\\tag{6.22}\\]\nwith the basis functions \\(\\phi_n\\) being the indicator polynomials \\(\\ell_n\\). We immediately see that the boundary flux term will simplify hugely, as the only term that is non-zero at \\(x=-1\\) comes from the product of \\(\\ell_0(-1) \\ell_0(-1)\\), using the convention that \\(\\xi_0 = -1\\), as \\(\\ell_n(-1) = 0\\) for \\(n \\ne 0\\). Similarly the only term that is non-zero at \\(x=+1\\) comes from the product of \\(\\ell_m(-1) \\ell_m(-1)\\). Therefore, for any number of modes \\(m\\), we only need to communicate one piece of information from the neighbouring element in order to solve the Riemann problem, and this is the value of the solution at that interface.\nHowever, by choosing as a basis the indicator polynomials \\(\\ell_n(x)\\), the resulting mass matrix will not be the identity, as the indicator polynomials are not orthogonal. The properties of the mass matrix will now crucially depend on how we choose the locations of the nodes, \\(\\xi_i\\). This is most easily done by linking the nodal form of (Equation 6.20) to the modal form (Equation 6.5), where here we are thinking of \\(\\phi_n\\) as being a different basis (\\(\\phi_n \\ne \\ell_n\\)) which is known to be well behaved. This implicitly allows us to restrict \\(\\xi_j\\).\nBy evaluating both forms at a node \\(\\xi_j\\) we get\n\\[\n  \\Psi_j = \\sum_n \\phi_n(\\xi_j) \\hat{\\Psi}_n.\n\\tag{6.23}\\]\nBy defining a (generalized) Vandermonde matrix \\(\\hat{V}\\) as\n\\[\n  \\hat{V}_{jn} = \\phi_n(\\xi_j)\n\\tag{6.24}\\]\nwe see that we can translate from the modal state vector \\(\\symbf{\\hat{\\Psi}} = (\\hat{\\Psi}_0, \\dots, \\hat{\\Psi}_N)^T\\) to the nodal state vector \\(\\symbf{\\Psi} = (\\Psi_0, \\dots, \\Psi_N)^T\\) via the matrix equation\n\\[\n  \\hat{V} \\symbf{\\hat{\\Psi}} = \\symbf{\\Psi}.\n\\tag{6.25}\\]\nWe can also connect the basis functions \\(\\phi_n\\) to the interpolating polynomials \\(\\ell_i\\) via the Vandermonde matrix. Note that\n\\[\n\\begin{aligned}\n      && \\Psi(t, x) &= \\sum_n \\hat{\\Psi}_n \\phi_n(x) \\\\\n      && &= \\sum_i \\Psi_i \\ell_i(x) \\\\\n      && &= \\sum_i \\sum_n \\hat{V}_{in} \\hat{\\Psi}_n \\ell_i(x) \\\\\n      && &= \\sum_n \\sum_i \\hat{V}_{in} \\hat{\\Psi}_n \\ell_i(x) \\\\\n      \\implies && 0 &= \\sum_n \\hat{\\Psi}_n \\left( \\sum_i \\left[ \\hat{V}_{in} \\ell_i(x) - \\phi_n(x) \\right] \\right).\n\\end{aligned}\n\\tag{6.26}\\]\nThis immediately gives\n\\[\n  \\hat{V}_{in} \\ell_i(x) = \\phi_n(x)\n\\tag{6.27}\\]\nor, by thinking of the basis functions and interpolating polynomials as vectors,\n\\[\n  \\hat{V}^T \\symbf{\\ell}(x) = \\symbf{\\phi}(x).\n\\tag{6.28}\\]\nThis allows us to convert the modal approach to deriving a scheme to a nodal approach directly through the Vandermonde matrix.\nTo construct the nodal scheme we need to fix the location of the nodal points. We have constructed the modal scheme to be well conditioned by looking at the mass matrix. This suggests that to make the nodal scheme well behaved we should ensure good conditioning of the Vandermonde matrix. This requires carefully choosing the nodes \\(\\xi_i\\). We also want to ensure that two of the nodes are at \\(x = \\pm 1\\), and that the accuracy of the scheme is as good as possible. All these conditions combine to suggest that the nodes \\(\\xi_i\\) should be given by the points, which are the zeros of \\(P_N'(x)\\) combined with \\(\\pm 1\\).\n\n\n\n\n\n\n\n\nFigure 6.1: The grid for a Discontinuous Galerkin method is split into cells or elements as indicated by the vertical dashed lines – here there are only \\(4\\) cells. Within each cell the solution is represented by an \\(m^{  ext{th}}\\) order polynoial, as shown by the dashed lines. This representation is central to the modal DG method. Equivalent information can be stored at specific nodes, as shown by the markers. Note how the number and location of the nodes varies with \\(m\\).\n\n\n\n\n\nFigure (Figure 6.1) shows the nodes and modes for a sine wave represented by a Discontinuous Galerkin method on a grid with only \\(4\\) cells. We see how rapidly the representation appears to converge to the smooth sine wave with increasing \\(m\\). Note also how the locations of the nodes varies with \\(m\\), as the optimal nodes changes with the order of the method. However, in all cases there are nodes at the boundaries of each cell.\n\nExercise 6.1 Construct the Vandermonde matrix converting modal coefficients, based on orthonormal Legendre polynomials, to nodal coefficients, based on Gauss-Lobatto nodal points, on the interval \\(x \\in [-1, 1]\\). For example, for \\(m=2\\) the result is, to \\(4\\) significant figures,\n\\[\n  V = \\begin{pmatrix}\n        0.7071 & -1.225 & 1.581 \\\\\n        0.7071 & 0 & -0.7906 \\\\\n        0.7071 & 1.225 & 1.581\n      \\end{pmatrix}.\n\\tag{6.29}\\]\nUsing the Vandermonde matrix and its inverse, check that you can convert from nodes to modes and vice versa. Check that the condition number grows slowly with \\(m\\) (roughly as \\(m^{1/2}\\) for large \\(m\\)).\n\nWith these restrictions, we can now construct the nodal scheme. As noted above, this scheme remains a modal scheme as generally introduced in (Section 6.1), but the basis functions are the indicator polynomials \\(\\ell_n(x)\\). Thus the scheme can be written in the mass matrix form as in (Equation 6.13) of\n\\[\n  \\hat{M} \\frac{\\partial \\symbf{\\hat{\\Psi}}}{\\partial t} + \\hat{S}^T u \\symbf{\\hat{\\Psi}} = -\\left[ \\symbf{\\phi F} \\right]_{-1}^1,\n\\tag{6.30}\\]\nbut now the two matrices are given by\n\\[\n\\begin{aligned}\n  \\hat{M}_{ln} &= \\int_{-1}^1 \\ell_l(x) \\ell_n(x), \\\\\n  \\hat{S}_{ln} &= \\int_{-1}^1 \\ell_l(x) \\frac{\\partial \\ell_n}{\\partial x}(x).\n\\end{aligned}\n\\tag{6.31}\\]\nBy using the Vandermonde matrix to link the nodal basis to an orthogonal basis such as the Legendre polynomials we can simplify the mass matrix to\n\\[\n  \\hat{M} = \\left( \\hat{V} \\hat{V}^T \\right)^{-1}.\n\\tag{6.32}\\]\nThe stiffness matrix can also be simplified, by re-writing \\(\\frac{\\partial \\ell_n}{\\partial x}(x)\\) as an expansion in terms of \\(\\ell_n(x)\\). Defining the differentiation matrix \\(\\hat{D}\\) as\n\\[\n  \\hat{D}_{ln} = \\left. \\frac{\\partial \\ell_n}{\\partial x}(x) \\right|_{x = \\xi_l}\n\\tag{6.33}\\]\nwe have \\(\\frac{\\partial \\ell_n}{\\partial x}(x) = \\sum_k \\hat{D}_{kn} \\ell_k(x)\\)\n\\[\n\\begin{aligned}\n    \\hat{S}_{ln} &= \\int_{-1}^1 \\ell_l(x) \\frac{\\partial \\ell_n}{\\partial x}(x) \\\\\n    &= \\int_{-1}^1 \\ell_l(x) \\sum_k \\hat{D}_{kn} \\ell_k(x) \\\\\n    &= \\sum_k \\left( \\ell_l(x) \\ell_k(x) \\right) \\hat{D}_{kn} \\\\\n    &= \\hat{M}_{lk} \\hat{D}_kn.\n\\end{aligned}\n\\tag{6.34}\\]\nThis shows that the stiffness matrix simplifies to\n\\[\n  \\hat{S} = \\hat{M} \\hat{D}.\n\\tag{6.35}\\]\nFinally, using similar methods to the steps above, we can link the differentiation matrix back to the Vandermonde matrix, via\n\\[\n  \\hat{D} = \\left( \\frac{\\partial V}{\\partial x} \\right) \\hat{V}^{-1}.\n\\tag{6.36}\\]\nThis is primarily useful when the modal function basis is a standard library function such as the (normalized) Legendre polynomials. This means that the basis functions and their derivatives, and hence the Vandermonde matrix and its derivatives, can be written solely in terms of library functions. For example, in Python the package contains (in ) the functions (which evaluates the Legendre polynomials), (which links the derivatives of the Legendre polynomials back to the Legendre polynomials themselves), and (which evaluates the Vandermonde matrix directly, but in un-normalized form).\nThere is one final step needed to construct the full scheme. So far, the method has been built assuming a single element with the coordinates \\(x \\in [-1, 1]\\). For most cases we will want to use a “small” number of modes, say \\(m \\le 5\\), and split the domain into \\(N\\) elements, like the cells in a finite volume scheme. If we assume a general element has coordinates \\(x \\in [x_{j-1/2}, x_{j+1/2}]\\) with width \\(\\Delta x\\), then the form of the scheme remains the same:\n\\[\n  M \\frac{\\partial \\symbf{\\hat{\\Psi}}}{\\partial t} + S^T u \\symbf{\\hat{\\Psi}} = -\\left[ \\symbf{\\phi} F \\right]_{x_{j-1/2}}^{x_{j+1/2}}.\n\\tag{6.37}\\]\nHowever, the change of coordinates needs to be factored in. We can see how this works by looking at the integral definitions, such as (Equation 6.31). We see that the mass matrix transforms as\n\\[\n  M = \\frac{\\Delta x}{2} \\hat{M},\n\\tag{6.38}\\]\nbut that the stiffness matrix is unchanged.\n\nExercise 6.2 From the Vandermonde matrices constructed above, build the mass, differentiation and stiffness matrices \\(\\hat{M}, \\hat{D}, \\hat{S}\\), on the interval \\(x \\in [-1, 1]\\). For example, for \\(m=2\\) the results are, to \\(4\\) significant figures,\n\\[\n\\begin{aligned}\n  \\hat{M} &= \\begin{pmatrix}\n        0.2667 & 0.1333 & -0.0667 \\\\\n        0.1333 & 1.067 & 0.1333 \\\\\n        -0.06667 & 0.1333 & 0.2667\n      \\end{pmatrix}, \\\\\n  \\hat{D} &= \\begin{pmatrix}\n        -1.5 & 2 & -0.5 \\\\\n        -0.5 & 0 & 0.5 \\\\\n        0.5 & -2 & 1.5\n      \\end{pmatrix}, \\\\\n  \\hat{S} &= \\begin{pmatrix}\n        -0.5 & 0.6667 & -1.667 \\\\\n        -0.6667 & 0 & 0.6667 \\\\\n        1.667 & -0.6667 & 0.5\n      \\end{pmatrix}.\n\\end{aligned}\n\\tag{6.39}\\]\n\n\n\n\n\n\n\n\n\nFigure 6.2: A Discontinuous Galerkin method with \\(m=3\\) and \\(16\\) elements applied to the advection equation, where a sine wave is advected once around the domain. Even at this low resolution the result is visually exact. The solutions are plotted at the nodal values, which are not evenly spaced.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: A Discontinuous Galerkin method with \\(m=3\\) and \\(16\\) elements applied to the advection equation, where a discontinuous top hat function is advected once around the domain. The expected Gibbs oscillations are seen.\n\n\n\n\n\nBy combining the nodal DG update described above with a time integrator we can look at the performance of the scheme. We need to take care in choosing the timestep. From the nodal point of view we can see that the width of the cell, \\(\\Delta x\\), is not going to be the limiting factor. Instead, the smallest distance between the (unequally spaced!) nodes is going to be crucial. General results (see e.g. (Hesthaven 2017)) suggest that reducing the timestep by a factor of \\(2 m + 1\\) is sufficient to ensure stability, but it does increase computational cost.\nFigures (Figure 6.2) and (Figure 6.3) show the advection of two initial profiles one period around a periodic domain. In (Figure 6.2) we see the excellent performance when applied to a smooth profile. The method is essentially indistinguishable from the exact solution. However, in (Figure 6.3), we see that when the method is applied to a discontinuous initial profile then Gibbs oscillations result. The only “nice” feature of the Discontinuous Galerkin method here is that these oscillations are confined to the elements next to the discontinuities, and do not spread to cover the entire grid.\nAs with finite difference schemes, there are a range of modifications that can be made to limit or eliminate these oscillations. In Discontinuous Galerkin methods it is typical to do this in two steps: first, identify which elements need limiting, and second, modify the data in the required cells. The identification step can be done using the nodal values: construct limited slopes from cell average values and compare the predicted values at cell boundaries to the nodal values actually stored. The modification step can be done in many ways. A number are outlined in (Hesthaven 2017).\n\n\n\n\n\n\n\n\nFigure 6.4: The convergence rate of the DG method applied to a sine wave. An explicit SSP third order Runge-Kutta time integrator is used. The expected convergence rate (\\(m+1\\)) is seen until the limits of the time integrator are reached.\n\n\n\n\n\nWith smooth solutions we can check the convergence rate of the method. In (Figure 6.4) the smooth sine profile is again advected once around a periodic domain, using mode numbers \\(m = 1, \\dots, 4\\), and checking convergence with the number of elements. This using a third order Runge Kutta method in time, and eventually the time integration error dominates over the spatial error.\n\n\n/Users/ih3/anaconda3/envs/mfc/lib/python3.12/site-packages/scipy/integrate/_ode.py:431: UserWarning: dop853: step size becomes too small\n  self._y, self.t = mth(self.f, self.jac or (lambda: None),\n\n\n\n\n\n\n\n\nFigure 6.5: The convergence rate of the DG method applied to a sine wave. An eighth order Runge-Kutta time integrator is used. The expected convergence rate (\\(m+1\\)) is seen even at high orders.\n\n\n\n\n\nIn (Figure 6.5) an \\(8^{\\text{th}}\\) order time integrator is used. This reduces the time integrator error far below what is needed, and we now see that every scheme converges at the expected rate. In more complex systems in multiple dimensions the error from the spatial terms will be much larger, and so lower order methods can be used without compromising the accuracy.\n\nExercise 6.3 Write a Python code to advect a profile around \\(x \\in [0, 1]\\) using the Discontinuous Galerkin method. A variety of Python packages can be used to facilitate this: numpy will solve linear systems and construct the matrices linked to Legendre polynomials, scipy.integrate will solve the ODE in time, and quadpy will construct the Gauss-Lobatto integration points.\nCheck that the solution converges as expected.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Flexibility with efficiency</span>"
    ]
  },
  {
    "objectID": "finite_elements_4.html#discussion",
    "href": "finite_elements_4.html#discussion",
    "title": "6  Flexibility with efficiency",
    "section": "6.4 Discussion",
    "text": "6.4 Discussion\nIn some ways Discontinuous Galerkin type methods seem a half-way-house between spectral methods and finite difference or finite volume methods. In principle the number of modes used within each element can be increased arbitrarily, giving the extremely rapid convergence of a spectral method. However, each element is linked to its neighbour, so there is still the communication with neighbouring points as in, for example, finite difference methods.\nThe key advantage of Discontinuous Galerkin methods comes with the latest computing hardware. These “Exascale” High Performance machines will rely on codes using very large numbers of relatively cheap, energy efficient individual computing cores (nearly always GPUs). This means the calculation must be performed in parallel across millions (or more) different compute cores. In this situation the limiting factor will be the communication with neighbouring points. This makes pure spectral methods totally impractical, and high accuracy finite difference methods (that have to communicate with many neighbours) will also not reach the performance expectations. As Discontinuous Galerkin methods only have to compute with one neighbouring element on each side, they minimise communication whilst giving high accuracy.\nHowever, the stiffness and mass matrices involved in the update grow rapidly with the number of spatial dimensions and with the size of the system to solve. In addition, simple Discontinuous Galerkin methods struggle with steep gradients and discontinuities. The complexity and cost of making these methods practical means that they are - as yet - rarely used. Future computing hardware considerations may make them increasingly important.\n\n\n\n\nHesthaven, Jan S. 2017. Numerical Methods for Conservation Laws: From Analysis to Algorithms. SIAM.",
    "crumbs": [
      "Finite Elements",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Flexibility with efficiency</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Boyd, John P. 2001. Chebyshev and Fourier Spectral Methods.\nDover Books in Mathematics.\n\n\nDurran, Dale R. 2010. Numerical Methods for Fluid Dynamics: With\nApplications to Geophysics. Vol. 32. Springer Science &\nBusiness Media.\n\n\nHesthaven, Jan S. 2017. Numerical Methods for Conservation Laws:\nFrom Analysis to Algorithms. SIAM.\n\n\nHughes, T. J. R. 2012. The Finite Element Method: Linear Static and\nDynamic Finite Element Analysis. Dover Civil and Mechanical\nEngineering. Dover Publications.\n\n\nLeVeque, Randall J. 1992. Numerical Methods for Conservation\nLaws. Vol. 214. Springer.\n\n\n———. 2002. Finite Volume Methods for Hyperbolic Problems. Vol.\n31. Cambridge University Press.\n\n\nToro, Eleuterio F. 2013. Riemann Solvers and Numerical Methods for\nFluid Dynamics: A Practical Introduction. Springer Science &\nBusiness Media.\n\n\nTrefethen, Lloyd Nicholas. 1996. “Finite Difference and Spectral\nMethods for Ordinary and Partial Differential Equations.” https://people.maths.ox.ac.uk/trefethen/pdetext.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix_background.html",
    "href": "appendix_background.html",
    "title": "Appendix A — Background material",
    "section": "",
    "text": "A.1 Taylor expansions\nGiven a function \\(f(x)\\) with sufficient derivatives at a point \\(X\\), the function can be represented as a polynomial using a Taylor series about the point \\(X\\). There are multiple forms of interest.\nThe general form with remainder term obscured is\n\\[\nf(x) = f(X) + (X - x) f'(X) + \\mathcal{O}\\left( (X - x)^2 \\right) \\, .\n\\tag{A.1}\\]\nThe series expanded as a general polynomial is\n\\[\nf(x) = \\sum_{k=0} \\frac{(X - x)^k}{k!} f^{(k)}(X)\n\\tag{A.2}\\]\nwhere the notation \\(f^{(k)}(X)\\) corresponds to the \\(k^\\text{th}\\) derivative of \\(f\\) evaluated at \\(X\\).\nFor finite differencing it is useful to restrict to an evenly spaced grid \\(x_j = x_0 + j \\, \\Delta x\\). Then, expanding about \\(x_i\\), we have\n\\[\nf(x_j) = \\sum_{k=0} \\frac{(j - i)^k \\, \\Delta x^k}{k!} f^{(k)}(x_i) \\, .\n\\tag{A.3}\\]\nThe most useful results, written out explicitly to low orders, are\n\\[\nf(x_{i \\pm 1}) = f(x_i) \\pm \\Delta x \\, f'(x_i) + \\frac{\\Delta x^2}{2} f''(x_i) \\pm \\frac{\\Delta x^3}{6} f'''(x_i) + \\mathcal{O}(\\Delta x^4)\\, .\n\\tag{A.4}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Background material</span>"
    ]
  },
  {
    "objectID": "appendix_background.html#sec-appendix-background-series",
    "href": "appendix_background.html#sec-appendix-background-series",
    "title": "Appendix A — Background material",
    "section": "A.2 Series expansions",
    "text": "A.2 Series expansions\n\nA.2.1 Fourier Series\nAn \\(L_2\\) square integrable periodic function of one variable \\(x\\) defined on \\([-\\pi, \\pi]\\) can be represented by the complex Fourier series\n\\[\nf(x) \\sim \\sum_{n=-\\infty}^{\\infty} a_n \\exp(i n x)\n\\tag{A.5}\\]\nwhere the (complex) coefficients \\(a_n\\) are given by\n\\[\na_n = \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} f(x) \\exp(-i n x) \\, \\text{d}x \\, .\n\\tag{A.6}\\]\nWhen the function depends on more than one variable the series expansion can be performed separately for each, or the dependency on the additional variables can be retained in the coefficient. Typically spatial dependence is expanded in a series and time dependence is retained in the coefficients, for example as\n\\[\nf(x, y, t) \\sim \\sum_{n_x=-\\infty}^{\\infty} \\sum_{n_y=-\\infty}^{\\infty} a_n(t) \\exp(i n_x x) \\exp(i n_y y) \\, .\n\\tag{A.7}\\]\n\n\nA.2.2 Eigenfunctions\nOne key use for series expansions is in differential equations. Each individual mode \\(f_n(x) = \\exp(i n x)\\) is an eigenfunction of both the first and second derivative operators,\n\\[\n\\begin{aligned}\n\\partial_x f_n(x) &= i n \\exp(i n x) \\\\ &= i n f_n(x) \\, , \\\\\n\\partial_{xx} f_n(x) &= -n^2 \\exp(i n x) \\\\ &= -n^2 f_n(x) \\, .\n\\end{aligned}\n\\tag{A.8}\\]\nWhen problems are linear the action of the differential operator can be studied as an algebraic operation on the individual modes.\nThis generalizes to more complex problems. Sturm-Liouville theory looks at problems where the spatial differential operator has the form\n\\[\n\\mathcal{L}(y) = \\frac{\\text{d}}{\\text{d}x} \\left[ p(x) \\frac{\\text{d} y}{\\text{d}x} \\right] + q(x) y \\, .\n\\tag{A.9}\\]\nHere \\(p, q\\) are known functions. The eigenfunctions of this operator obey\n\\[\n\\mathcal{L}(y_n) = -\\lambda_n w(x) y_n\n\\tag{A.10}\\]\nwhere \\(w\\) is a known weighting function. With these eigenfunctions, an arbitrary function can be represented as\n\\[\nf(x) \\sim \\sum_{n=1}^\\infty a_n y_n(x)\n\\tag{A.11}\\]\nwhere the coefficients \\(a_n\\) can be explicitly computed as\n\\[\na_n = \\int_{-\\pi}^{\\pi} f(x) y_n(x) w(x) \\, \\text{d}x \\, .\n\\tag{A.12}\\]\nThis is an expansion in terms of orthogonal functions, using that\n\\[\n\\int_{-\\pi}^{\\pi} y_m(x) y_n(x) w(x) \\, \\text{d}x = \\delta_{mn} \\, .\n\\tag{A.13}\\]\n\n\nA.2.3 Key examples\nIn the Fourier Series case \\(p(x) = 1\\), \\(q(x) = 0\\), and \\(w(x) = 2 \\pi\\). The eigenvalues are \\(\\lambda_n = n^2\\).\nIn the Legendre equation case (where the domain is conventionally \\(x \\in [-1, 1]\\)) \\(p(x) = 1 - x^2\\), \\(q(x) = 0\\), and \\(w(x) = (2 n + 1) / 2\\). The eigenvalues are \\(\\lambda_n = n (n + 1)\\).\nSpherical harmonics \\(Y^m_\\ell (\\theta, \\varphi)\\) are eigenfunctions of the covariant second derivative operator on a spherical shell with radius \\(r\\), so that\n\\[\nr^2 \\nabla^2 Y^m_\\ell = -\\ell (\\ell + 1) Y^m_\\ell \\, .\n\\tag{A.14}\\]\nSpherical harmonics look like a Fourier mode in \\(\\varphi\\) multiplied by an eigenfunction of the (associated) Legendre equation in \\(\\theta\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Background material</span>"
    ]
  },
  {
    "objectID": "appendix_order.html",
    "href": "appendix_order.html",
    "title": "Appendix B — Order of accuracy",
    "section": "",
    "text": "B.1 Notation\nAssume that we are given a PDE with appropriate initial boundary conditions. We denote the solution to the PDE, using initial data at \\(t=0\\) of \\(y_0(x)\\), as\n\\[\ny\\left(x, t \\vert y_0(x; t=0)\\right) \\, .\n\\tag{B.1}\\]\nWe will assume that the PDE is well posed, in the sense that a small perturbation in the initial conditions is bounded. That is, we assume that for all small numbers \\(\\epsilon\\) and for all perturbations \\(\\delta\\) there exists a \\(K\\) independent of time such that\n\\[\n\\| y(x, t \\vert y_0(x; t=0) + \\delta) - y(x, t \\vert y_0(x; t=0)) \\| \\le \\delta e^{K t} \\, .\n\\tag{B.2}\\]\nNext we assume that we are constructing a numerical approximation to the solution \\(\\mathtt{y}(x, t)\\). It is possible that this solution is only constructed at a finite number of points (for example, a finite difference method on a grid), or it might be constructed at finitely many time intervals but be computable everywhere in space (as in a spectral or finite element method). We denote the numerical method’s update scheme by\n\\[\n\\mathtt{y}\\left(x, t^{n+1} \\vert \\left\\{ \\mathtt{y}(x, t^n) \\right\\} \\right) = \\mathcal{F} \\left( \\left\\{ \\mathtt{y}(x, t^n) \\right\\} \\right) \\, .\n\\tag{B.3}\\]\nEven an implicit method can (in principle) be written in this form, but it is simplest to think of this as an explicit method.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Order of accuracy</span>"
    ]
  },
  {
    "objectID": "appendix_order.html#sec-order-accuracy-LTE",
    "href": "appendix_order.html#sec-order-accuracy-LTE",
    "title": "Appendix B — Order of accuracy",
    "section": "B.2 Local truncation error",
    "text": "B.2 Local truncation error\nThe easiest analysis of the accuracy of a method looks at the error introduced over a single step, assuming that the input data is exact. This is the Local Truncation Error \\(\\epsilon_{n+1}\\), given by\n\\[\n\\epsilon_{n+1} = \\left\\| y(x, t^{n+1} \\vert y_0(x; t=0) ) - \\mathtt{y}\\left(x, t^{n+1} \\vert \\left\\{ y(x, t^n \\vert y_0(x; t=0)) \\right\\} \\right) \\right\\| \\, .\n\\tag{B.4}\\]\nWhen computing the local truncation error it is usual to use a series expansion, such as a Taylor series expansion, in terms of small quantities controllable by the numerical method. Typical examples would be the grid spacings \\(\\Delta t, \\, \\Delta x\\) in finite difference methods, and the inverse of the number of degrees of freedom \\(N^{-1}\\) in spectral or finite element methods. We then assume that only the first term in the expansion survives, giving (for example)\n\\[\n\\epsilon_{n+1} \\propto \\Delta t^{p+1} \\, .\n\\tag{B.5}\\]\nThis makes the local order of accuracy be \\(p+1\\). We typically assume that \\(p\\) is independent of time (and hence independent of \\(n\\)), and write the local truncation error as \\(\\epsilon\\) by maximising over all time.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Order of accuracy</span>"
    ]
  },
  {
    "objectID": "appendix_order.html#sec-order-accuracy-GTE",
    "href": "appendix_order.html#sec-order-accuracy-GTE",
    "title": "Appendix B — Order of accuracy",
    "section": "B.3 Global truncation error",
    "text": "B.3 Global truncation error\nLocal truncation error is the easiest thing to analyse, but not what we want to compute. We want to relate the error at the end of the simulation, when it has taken many steps, to the parameters (such as grid spacing). This is the Global Truncation Error\n\\[\n\\mathcal{E}_{T} = \\left\\| y(x, T \\vert y_0(x; t=0)) - \\mathtt{y}\\left(x, T \\vert \\left\\{ \\mathtt{y}(x, T - \\Delta t) \\right\\} \\right) \\right\\| \\, .\n\\tag{B.6}\\]\nThis cannot be directly computed using the local truncation error, as the numerical solution at time \\(T\\) is not computed from the exact solution at the previous timestep, but from the approximate numerical solution at that point. However, we can add and subtract the exact solution with different initial conditions, at the time \\(T - \\Delta t\\). Thus\n\\[\n\\begin{aligned}\n\\mathcal{E}_{T} &= \\left\\| y(x, T \\vert y_0(x; t=0)) - y(x, T \\vert \\mathtt{y}(x; T - \\Delta t)) + \\right. \\\\ &\\quad \\left. y(x, T \\vert \\mathtt{y}(x; T - \\Delta t)) - \\mathtt{y}\\left(x, T \\vert \\left\\{ \\mathtt{y}(x, T - \\Delta t) \\right\\} \\right) \\right\\| \\\\\n&\\le \\left\\| y(x, T \\vert y_0(x; t=0)) - y(x, T \\vert \\mathtt{y}(x; T - \\Delta t)) \\right\\| + \\\\ &\\quad \\left\\| y(x, T \\vert \\mathtt{y}(x; T - \\Delta t)) - \\mathtt{y}\\left(x, T \\vert \\left\\{ \\mathtt{y}(x, T - \\Delta t) \\right\\} \\right) \\right\\| \\\\\n&\\le \\left\\| y(x, T \\vert y_0(x; t=0)) - y(x, T \\vert \\mathtt{y}(x; T - \\Delta t)) \\right\\| + \\epsilon \\\\\n&\\le \\left\\| y(x, T \\vert y(x; t=T - \\Delta t)) - y(x, T \\vert \\mathtt{y}(x; T - \\Delta t)) \\right\\| + \\epsilon \\\\\n&\\le \\mathcal{E}_{T-\\Delta t} e^{K \\, \\Delta t} + \\epsilon \\, .\n\\end{aligned}\n\\tag{B.7}\\]\nHere we have first used the definition of the local truncation error, then the definition of well-posedness.\nUsing this recursion relation, combined with the fact that the global truncation error after a single step is precisely the local truncation error, we find that\n\\[\n\\mathcal{E}_T \\propto \\frac{\\epsilon}{\\Delta t} \\, .\n\\tag{B.8}\\]\nThis result says that if the local truncation error has order of accuracy \\(p+1\\), then the global truncation error has order of accuracy \\(p\\). Loosely, this can be understood as the finite time \\(T\\) requires \\(\\sim \\Delta t^{-1}\\) timesteps to reach, and each of those steps introduces an error \\(\\sim \\epsilon\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Order of accuracy</span>"
    ]
  },
  {
    "objectID": "appendix_lax.html",
    "href": "appendix_lax.html",
    "title": "Appendix C — Lax Equivalence Theorem",
    "section": "",
    "text": "C.1 Banach spaces\nA Banach space is a complete normed vector space. For our purposes, this will be the space of discrete approximations to the PDE. The norm will be some measure of the “size” of the solution, such as the infinity norm\n\\[\n\\| \\symbf{\\phi} \\|_\\infty = \\max_i \\phi_i \\, ,\n\\tag{C.4}\\]\nor the 2-norm\n\\[\n\\| \\symbf{\\phi} \\|_2 = \\sum_i \\sqrt{ (\\phi_i)^2 \\, \\Delta x } \\, .\n\\tag{C.5}\\]\nThis allows us to formally, but abstractly, state our two key conditions and our goal.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Lax Equivalence Theorem</span>"
    ]
  },
  {
    "objectID": "appendix_lax.html#consistency",
    "href": "appendix_lax.html#consistency",
    "title": "Appendix C — Lax Equivalence Theorem",
    "section": "C.2 Consistency",
    "text": "C.2 Consistency\nA numerical approximation is consistent if it correctly represents the PDE in the continuum limit. Formally, let \\(\\symbf{\\phi}\\) be the exact solution to the PDE, sampled onto the discrete grid as appropriate. Also let \\(\\mathcal{L}\\) be the formal operator that evolves the exact solution forward in time. Therefore\n\\[\n\\mathcal{L}(T) \\symbf{\\phi}(0) = \\symbf{\\phi}(T) \\, ,\n\\tag{C.6}\\]\nand\n\\[\n\\mathcal{L}(\\Delta t) \\symbf{\\phi}(T) = \\symbf{\\phi}(T + \\Delta t) \\, .\n\\tag{C.7}\\]\nThen the numerical scheme is consistent if\n\\[\n\\lim_{\\Delta t \\to 0} \\left\\| \\left(\\mathcal{L}(\\Delta t) - \\mathcal{L}_{\\Delta t} \\right) \\symbf{\\phi}(T) \\right\\| \\to 0 \\, .\n\\tag{C.8}\\]\nFor later purposes we write that the scheme is consistent if there exists a constant \\(K \\in [0, \\infty)\\) such that\n\\[\n\\left\\| \\left( \\mathcal{L}(\\Delta t) - \\mathcal{L}_{\\Delta t} \\right) \\symbf{\\phi}(T) \\right\\| \\le  K_{C} \\, \\Delta t\n\\tag{C.9}\\]\nand \\(K_C\\) is independent of \\(\\Delta t\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Lax Equivalence Theorem</span>"
    ]
  },
  {
    "objectID": "appendix_lax.html#stability",
    "href": "appendix_lax.html#stability",
    "title": "Appendix C — Lax Equivalence Theorem",
    "section": "C.3 Stability",
    "text": "C.3 Stability\nA numerical approximation is stable if it does not blow up “too fast”. This needs some care, as it is possible that the exact solution to the PDE grows quickly, even exponentially or instantaneously in special cases. Therefore the most general stability criteria that we can work with is to state that the scheme is stable at time \\(T = N \\, \\Delta t\\) if\n\\[\n\\left\\| \\left( \\mathcal{L}_{\\Delta t} \\right)^N \\symbf{\\phi}(0) \\right\\| \\le K_T \\left\\| \\symbf{\\phi}(0) \\right\\| + D \\, \\Delta t \\, .\n\\tag{C.10}\\]\nAgain, the constants \\(K_T, D \\in [0, \\infty)\\) and must be independent of \\(\\Delta t\\) (although they can depend on the finite time \\(T\\)). The term involving \\(K_T\\) ensures that the numerical approximation does not grow too fast compared to the true solution, but does allow for bounded growth. The term involving \\(D\\) allows for purely numerical growth even if the true solution is bounded, but ensures that any such growth converges to zero in the continuum limit.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Lax Equivalence Theorem</span>"
    ]
  },
  {
    "objectID": "appendix_lax.html#convergence",
    "href": "appendix_lax.html#convergence",
    "title": "Appendix C — Lax Equivalence Theorem",
    "section": "C.4 Convergence",
    "text": "C.4 Convergence\nOur goal, via the Lax Equivalence Theorem, is to show that the continuum limit of the numerical scheme is the true solution. This means the numerical scheme converges. The scheme is said to be convergent at time \\(T = N \\, \\Delta t\\) if\n\\[\n\\lim_{\\Delta t \\to 0} \\left\\| \\left( \\mathcal{L}(T) - \\left( \\mathcal{L}_{\\Delta t} \\right)^N \\right) \\symbf{\\phi}(0) \\right\\| \\to 0 \\, .\n\\tag{C.11}\\]\nMore precisely, the scheme converges if there exists a constant \\(K \\in [0, \\infty)\\) such that\n\\[\n\\left\\| \\left( \\mathcal{L}(T) - \\left( \\mathcal{L}_{\\Delta t} \\right)^N \\right) \\symbf{\\phi}(0) \\right\\| \\le K \\, \\Delta t.\n\\tag{C.12}\\]\nAgain, the constant \\(K\\) must be independent of \\(\\Delta t\\).\nThe key point that distinguishes convergence from consistency is the number of steps taken by the discrete scheme. To be consistent the error needs to converge over a single discrete step. For convergence the error needs to converge after \\(N\\) steps at fixed \\(T\\), and in the continuum limit \\(\\Delta t \\to 0\\) this means \\(N \\to \\infty\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Lax Equivalence Theorem</span>"
    ]
  },
  {
    "objectID": "appendix_lax.html#lax-equivalence-theorem",
    "href": "appendix_lax.html#lax-equivalence-theorem",
    "title": "Appendix C — Lax Equivalence Theorem",
    "section": "C.5 Lax Equivalence Theorem",
    "text": "C.5 Lax Equivalence Theorem\nWith the formal machinery set up, the Lax theorem follows.\n\nTheorem C.1 (Lax Equivalence Theorem) Given a well-posed, linear, initial value problem, and a consistent numerical scheme to that problem, stability is necessary and sufficient for convergence.\n\n\nProof. First note that\n\\[\n\\begin{aligned}\n\\left\\| \\left( \\mathcal{L}(T) - \\left( \\mathcal{L}_{\\Delta t} \\right)^N \\right) \\symbf{\\phi}(0) \\right\\| &= \\left\\| \\left( \\mathcal{L}(\\Delta t) - \\mathcal{L}_{\\Delta t} \\right) \\symbf{\\phi}(T - \\Delta t) + \\right. \\\\ & \\qquad \\left. \\mathcal{L}_{\\Delta t} \\symbf{\\phi}(T - \\Delta t) - \\left( \\mathcal{L}_{\\Delta t} \\right)^N \\symbf{\\phi}(0)\\right\\| \\\\\n& \\le K_C \\, \\Delta t + \\left\\| \\mathcal{L}_{\\Delta t} \\left( \\mathcal{L}(T - \\Delta t) - \\left( \\mathcal{L}_{\\Delta t} \\right)^{N-1} \\right) \\symbf{\\phi}(0)\\right\\| \\\\\n& \\le K_C \\, \\Delta t + K_T \\left\\|  \\left( \\mathcal{L}(T - \\Delta t) - \\left( \\mathcal{L}_{\\Delta t} \\right)^{N-1} \\right) \\symbf{\\phi}(0) \\right\\| + \\\\ & \\quad D \\, \\Delta t.\n\\end{aligned}\n\\tag{C.13}\\]\nBy induction, and using that the initial error is zero, this allows us to write all terms as \\(\\propto \\Delta t\\) and hence say that consistency plus stability imply convergence.\nIn the other direction, it is immediate that convergence implies stability.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Lax Equivalence Theorem</span>"
    ]
  }
]